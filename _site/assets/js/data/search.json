[
  
  {
    "title": "Confidential Matching – A Breakthrough for Privacy-Conscious Advertising or a Temporary Fix?",
    "url": "/gunnargriese.github.io/posts/confidential-matching/",
    "categories": "Privacy",
    "tags": "privacy",
    "date": "2024-09-17 00:00:01 +0200",
    





    
    "snippet": "On September 12, 2024, Google announced a new privacy-focused feature called Confidential Matching aimed at advertisers using their Customer Match and Enhanced Conversions tools within the Google M...",
    "content": "On September 12, 2024, Google announced a new privacy-focused feature called Confidential Matching aimed at advertisers using their Customer Match and Enhanced Conversions tools within the Google Marketing Platform (GMP). This announcement comes amid the ever-rising concerns over privacy regulations in the EU (e.g., GDPR) and also increasingly in the US (e.g., CCPA), which push advertisers to handle customer data more securely. But is it truly a breakthrough, or does it merely obscure underlying concerns about data sharing?In this post, I’ll explain how Confidential Matching works, its role in Google’s marketing ecosystem, and critically assess whether it provides a real solution for privacy-focused advertisers. By the end, you’ll have a clearer understanding of whether this new feature is a significant step forward or “just” another method to maintain data control under the guise of privacy protection.What is Confidential Matching?As privacy regulations like GDPR and CCPA are enforced more and more, advertisers face increasing challenges when using customer data for ad targeting and measurement. Traditionally, tools like Customer Match and Enhanced Conversions required advertisers to share sensitive personal data, such as email addresses or phone numbers, with Google. Confidential Matching offers a new solution by allowing advertisers to match customer data with Google’s records without directly sharing that raw information.This is achieved using a technology called Trusted Execution Environments (TEEs), which act as secure “digital vaults”. Think of a TEE as a locked, isolated chamber where data can be processed without external access. According to Google, TEEs ensure that customer data remains secure while still allowing advertisers to gain insights and maintain advertising effectiveness.In short, Confidential Matching allows advertisers to benefit from matching their data with Google’s datasets without directly sharing private information—aiming to balance privacy with performance.What are Customer Match and Enhanced Conversions?Customer Match and Enhanced Conversions are two powerful tools in Google’s advertising suite, both relying on customer data to improve targeting and measurement. Confidential Matching is set to enhance these tools by allowing advertisers to match customer data with Google’s user base more securely, thanks to TEEs.But before we have a closer look at Confidential Matching, let’s first uncover what each of these tools does and why customer data is so crucial to their effectiveness.Customer Match - Target users that you knowFor effective targeted advertising, it is essential to identify subsets of users who are relevant to the business in one way or another. These users might be likely to buy soon, about to churn shortly, or about to exhibit another form of behavior. Businesses often use various data sources to create these segments and identify relevant subsets of users.One of the most valuable data sources is the customer data businesses have collected over time. This data can include email addresses, phone numbers, and behavioral data (e.g., purchase history, website visits and actions). Once advertisers have collected this data and identified relevant subsets of their customers, they can use Customer Match to upload the users’ contact information to Google. The upload can be done using manual CSV uploads, the respective APIs, or the Google Ads Data Manager (direct connection to various cloud data storage systems). Google also offers the option to encrypt the data using the SHA256 algorithm before uploading it.  It is important to note that ever since Google started enforcing its EU consent policy, advertisers need to ensure that they also pass the required consent signals.Customer Match - Basic ProcessGoogle then compares the uploaded customer data with its user database to find matches and creates a so-called “Customer Match audience” from all the matched entries. The data can also be used to create lookalike audiences, which are users who share characteristics similar to those of the uploaded customer list. The resulting audiences can then be attached to campaigns and ads with messaging directly related to this specific subset of users, which obviously allows for very targeted messaging.Users who are signed in to Google services (e.g., Gmail, YouTube), have given Google permission to use their data for this purpose, and match an entry in the uploaded customer list, which will then be exposed to the ads. This allows advertisers to target their existing customers with personalized ads and to reach new users who share similar characteristics with their existing customers.A pretty powerful tool, don’t you agree? Unfortunately, there’s a catch. To use this feature, advertisers need to hand over (consented) contact data of their customers to Google and rely on Google not using the data for any (not consented) purposes.Enhanced Conversions - Get better insights into your campaign performanceWhile Customer Match is a tool for delivering targeted advertising messaging, Enhanced Conversions is primarily concerned with enhancing the measurement of GMP campaign and ad performance. Both Enhanced Conversions and Customer Match share a common characteristic, though: They rely on the advertiser sharing customer contact data with Google, which Google then matches against its own database entries.Enhanced Conversions - Basic ProcessIn Enhanced Conversions, the customer data is used to more reliably attribute a user’s ad exposure or interaction with a conversion registered on the advertiser’s website. For Google signed-in users, Google can keep track of all ad impressions and clicks that a specific user had within their ad network. With the advertiser passing on the user’s contact details upon a conversion, Google can match its customer data again against the advertiser’s customer data.Matching these deterministic identifiers (e.g., eail addresses) allows Google to provide the advertiser with accurate reporting on their campaign performance. Google doesn’t have to rely on unstable third-party cookies for any matched user. Still, it can use the advertiser’s customer data to link ad exposure to business goals. Additionally, better attribution of ad exposure will allow Google’s algorithms to optimize the campaigns better, eventually yielding better results for the advertiser.As you can see, at the end of the day, the mechanism and the associated risks are similar to those of Customer Match. What differs is simply how the data is used within Google’s systems.Why Confidential Matching?With the erosion of third-party cookies and other tracking technologies, Google has pushed advertisers to adopt technologies that rely on them sharing more PII with Google. Especially the use of Enhanced Conversions and Customer Match has been promoted as the salvation for advertisers to continue measuring and targeting users across the web. Both technologies require the advertiser to share PII, such as their customers’ email addresses and phone numbers, with Google. Google then maps these data points against their user records (e.g., from Google Chrome logged-in users) to improve the measurability of campaign performance and enable retargeting use cases. While the results for businesses that adopted these methods have been nothing short of impressive, many businesses have been (rightfully) hesitant to share their customer data with Google due to privacy concerns.Confidential Matching for Customer Match (and soon for Enhanced Conversions) aims to provide a secure way for advertisers to match their customer data with Google’s own data in a “privacy-preserving” manner using Trusted Execution Environments (TEEs). With the introduction of Confidential Matching, Google seeks to address the ever-growing concerns around data privacy and security in digital advertising about Personal Identifiable Information (PII) - in this case, customer data like email addresses, phone numbers, etc.. especially in light of privacy law requirements like the GDPR in Europe and similar laws in the United States, like the CCPA.Before Confidential Matching, advertisers had to share all their customer data relevant to Customer Match or Enhanced Conversions with Google. Then, Google would use the data it could match within its downstream systems. For the (potentially large) share of unmatched user data, businesses had to trust Google to not further process that data for their own purposes. With Confidential Matching, which operates on TEEs, Google has found a way to match the data without ever seeing the original data itself.What are Trusted Execution Environments (TEEs)?A TEE—also referred to as “Confidential computing”—is an environment for executing code and processing data in a secure and isolated manner. Think of TEEs like a clean room where sensitive (or any other) data is processed in a way that the TEE operator (in this case, Google) cannot access it. One or multiple data owners send data to the system. The data is encrypted before it enters the TEE and finally decrypted within the TEE.The TEE ensures that the data is processed securely and that the results are only accessible to authorized parties. Furthermore, the TEE can provide attestation, which is cryptographic proof that the code running in the TEE is genuine and has not been tampered with. At the same time, the TEE is auditable, meaning that the data owner (or a third party) can verify that the data has been processed according to the requirements.Hence, the core characteristics of TEEs as implemented by Google can be summarized as follows:  Data Isolation: Google, as the operator of the TEE, cannot access the data processed within the TEE.  Attestation: The TEE provides cryptographic proof that the code running within it is genuine and has not been tampered with.  Auditable: The data owner can verify that the data has been processed according to the requirements.In general, TEEs are an exciting technology for enhancing data privacy and security, as they allow data owners to share their data without exposing each party’s actual data.Bringing it all together: Confidential MatchingSo, how does all of this fit together, and how does it “enhance privacy” (as Google claims)?There are two main problems with Customer Match and Enhanced Conversions. The first one is that, in most cases, advertisers share too much data with Google. They’ll pass on any customer data and hope that Google finds plenty of matches in their database to enhance their targeting and measurement. The second problem is that once advertisers share the data, they have no control over how Google processes and uses it. Google might use the valuable customer data for their own purposes (other than retargeting or measurement enhancements), for which the advertiser didn’t obtain valid user consent.Now, let’s examine how Confidential Matching using TEEs works and whether the issues are addressed adequately.Simplified flow diagram of Confidential MatchingAs you can see from the illustration above, both the advertiser and Google upload their customer data to the TEE, where the matching of the two data sources is orchestrated. The output of this matching process is only the data points for which there is a match in Google’s database. These matches are then made available in the GMP as audiences or for reporting purposes. In essence, Google cannot learn any new data from the advertiser since this is prohibited by the TEE’s functionality.ConclusionConfidential Matching seems to address a key issue: excessive data sharing. It offers a solution where only necessary, matched data is exposed, helping advertisers meet growing privacy requirements. By leveraging Trusted Execution Environments, advertisers can trust that Google won’t see or store their raw customer data.However, while this technical solution tackles data security, it doesn’t fully address purpose limitation—that is, whether Google might still use this matched data for purposes beyond retargeting or measurement. E.g., even if the advertiser ensures user consent for retargeting, once the data is in Google’s system, it’s unclear whether Google might utilize this data in ways the advertiser’s users haven’t explicitly agreed to. Now, one might argue that since Google had access to this data all along and (ideally) obtained user consent for their own usage purposes, the advertiser shouldn’t be concerned with this. But given Google’s vast ecosystem, this remains a valid concern for businesses mindful of privacy compliance.Moreover, advertisers using Confidential Matching still bear the legal and reputational risks if privacy regulations evolve or if regulators decide that these technical solutions aren’t compliant. For instance, privacy watchdogs (e.g., NYOB) might as well demand more guarantees about how matched data is used downstream.In light of these concerns, Confidential Matching is a step forward—but not a solution that should lead advertisers to blindly adopt it. It addresses data security but leaves room for doubt regarding the actual handling of data post-match. As privacy is top of mind for many users, businesses will still need to carefully evaluate their legal obligations, privacy risk profiles, and trust in Google’s systems.I hope you found this article insightful and that you have a better understanding of Google’s new Confidential Matching feature. If you have any further questions or need help handling the discussions internally, feel free to contact me via one of the channels listed on this website. Happy tracking!ReferencesLinks to relevant articles for further reading on the topic:  Google announcement  Google’s GitHub Repo on TEEs  Search Engine Land article  DigiDay article"
  },
  
  {
    "title": "GTM Server-Side Pantheon - Part 1 - Tapping into the Power of Firestore",
    "url": "/gunnargriese.github.io/posts/gtm-server-side-firestore-integrations/",
    "categories": "GTM",
    "tags": "gtm-server-side",
    "date": "2024-09-10 00:00:01 +0200",
    





    
    "snippet": "For those who have been following the evolution of GTM Server-Side (sGTM) and Google Cloud Platform (GCP) services, you might recall my early enthusiasm about the potential integrations. In a previ...",
    "content": "For those who have been following the evolution of GTM Server-Side (sGTM) and Google Cloud Platform (GCP) services, you might recall my early enthusiasm about the potential integrations. In a previous blog post about GTM Server-Side, I highlighted the prospect of enriching the clickstream data collected by GA4 with the help of other GCP services. Here is what I wrote about this way back in August 2020:  Furthermore, because the GTM container is executed in Google’s cloud environment, other GCP resources like BigQuery, ML Engine, and Cloud Functions will most likely be integrated soon. This will open up a lot of possibilities for advanced use cases involving machine learning and event-based analytics.Once this new GTM feature moves out of the beta phase after being improved based on feature requests and active community members’ contributions, even more possibilities will be available, significantly changing tracking implementations based on GTM.Obviously, a lot has changed since then. Cloud functions are now Cloud Run functions, ML Engine as such doesn’t exist anymore and has been integrated into the Vertex AI platform, and GTM Server-Side has been officially released out of beta.However, the vision of enriching the clickstream collected by GA4 with data provided by GCP services or feeding a company’s downstream systems in real time is still valid. In this series of blog posts, I want to invite you to join me on a mythological journey exploring the possibilities of integrating first-party data or triggering processes from sGTM using Firestore (this one), Pub/Sub (part 2), BigQuery (part 3), Google Sheets (part 4), and even Predictive Analytics &amp; GenAI (part 5).The state of GTM Server-SideSince the release of sGTM, we have come a long way, and more and more companies have decided to opt into the technology to take advantage of the additional data controls for, e.g., data quality or privacy reasons. This trend has been powered by three main factors from my perspective:  Google’s commitment to the technology and the continuous improvement of the service  Companies’ and users’ increasing demand for data control and privacy compliance (especially in Europe and moving forward increasingly also in the US)  Lower barriers of entry (more documentation and community support) and decreased need for maintaining server infrastructure (driven by players like Stape (first-mover), AddingWell, etc.)While many companies have migrated their GA4 data collection to sGTM and might even use it to send data to other vendors, the technology’s potential is still not fully exploited. As is often the case in our line of work, most businesses have been focusing on the data collection part and have not yet fully embraced the possibilities for data enrichment and activation that are at their fingertips.Drivers for GTM Server-Side adoptionThe true power of an intermediary collection endpoint like sGTM lies in its ability to act as a data hub, where data from various, business-relevant sources can be transformed, combined, and sent to other systems in real time. In other words, you can gather different kinds of data (and combine them) and then send the widened data to the marketing vendors’ or your own systems as you see fit.Standing of the Shoulders of Gods - The GTM Server-Side PantheonThis is where the sGTM Pantheon comes into play. The Pantheon is a collection of (technical) solutions provided by Google, which allows one to easily get data from external services into sGTM via API calls or send data to external (GCP) destinations. The idea is to enhance sGTM’s native capabilities to allow businesses “to improve their reporting, bidding, audience management, and data pipeline management and processes” (s. Google’s documentation).Admittedly, while not all solutions presented here are entirely new from a technical perspective, having these blueprints gathered in one place and explaining these wherever possible lowers the barrier of entry to take advantage of the full power of sGTM. So, this blog post series is my humble contribution to increasing adoption on a broader scale.How Dall-E thinks the Greek Pantheon looks likeNow, you might still ask yourself, why the “Greek Pantheon”? Well, Google decided to choose names from Greek mythology for each of the solutions. Just like the Greek gods, each solution has its own strengths and weaknesses, and they can be combined to create “god-like” data flows. Are you intrigued? I for sure was when I first stumbled upon the Pantheon and decided to keep the theme and mythology references for this series of posts.Integrate Firestore into GTM SS - A match made in heavenIn the first part of this series, I want to focus on a set of solutions that are concerned with integrating Firestore into sGTM. GCP’s Firestore in the context of sGTM is particularly interesting, because Firestore and sGTM are a perfect match for real-time data enrichment and gathering. Firestore’s fast, scalable NoSQL database architecture allows for dynamic data lookups, while sGTM is the intermediary for collecting and processing first-party data.Together, they enable businesses to enrich data streams on the fly, injecting further context into analytics, marketing, and personalization efforts. In the context of sGTM, Firestore is an additional storage option (alongside browser storage) that is fully controlled by the organization operating sGTM, that can be used to retrieve or store (potentially sensitive) first-party data, and that never exposes data to the browser (unless configured as such).Firestore integration into sGTMGTM Server-Side and Firestore - Some technical backgroundEver since the advent of the Firestore API for sGTM, it has become possible to tap into Firestore’s capabilities from within sGTM. In particular, the API comes with a set of methods allowing for reading, writing, and querying Firestore documents (using Firestore in Native mode):  Firestore.read - This function looks up data from a given Firestore document and returns a promise that resolves to an object returning the data.  Firestore.write - This function writes whatever data you specify to a Firestore document or collection.  Firestore.query - This function allows you to query a given collection and returns an array of Firestore documents that match specified query conditions.  Firestore.runTransaction - This function allows reading and writing from Firestore atomatically.Combined with the Promise API, these methods enable you to use these APIs asynchronously in variables (previously we had to build custom tags or clients), which can then be associated with any triggers or tags of your liking - making the integration of Firestore into sGTM a breeze (or least significantly easier).Now, let’s examine the gods (and one demigod) of the Greek Pantheon who are concerned with Firestore integrations and look at some actual use cases.Built-in Firestore Lookup Variable - Not so much a god, but a demigod nonethelessThe Firestore Lookup variable in sGTM may not have the full divine powers of the Greek Pantheon (and, therefore, is not part of it), but I certainly consider it a reliable demigod. It offers a powerful way to enrich your data streams, especially when you need a database with lightning-fast read/write operations to provide real-time data lookups and widen the information available to your marketing tools or other systems.To provide you with a practical example of how to widen the data available in GA4 with additional user data stored in Firestore, I created a simple “user database” in Firestore with the following structure and entries:Exemplary Firestore User DatabaseAs you can see, for each user in the database, we store the user_id, the customer_score, and the order_count. The user_id is the unique identifier for each user, while the customer_score and order_count are attributes that we want to use to enrich the data sent to GA4. The customer_score could be used to segment users based on their value to the business, while the order_count could be used to identify loyal customers or to trigger tags conditionally. In general, we unlock activation use cases in our downstream systems if we can get these data points (or similar ones) in there using the Lookup variable.The Firestore Lookup variable is designed to fetch data from Firestore documents, which are structured in collections (in case you’re unfamiliar with the Firestore data model, you can read more about here). Its flexibility allows for two types of lookups: direct document path or collection-based query. When using the direct path method, you can pull specific documents by providing their exact location in the database (e.g., user_data/Event Data - user_id). This is the fastest approach, as it immediately fetches the desired document without further filtering. In the example below, the variable would retrieve the user_data document with the user_id matching the value from the GA4 event data and return the user’s customer_score.The Firestore Lookup Types in GTM Server-SideFurthermore, Firestore Lookup variables allow for collection-based queries. By querying collections based on document fields (e.g., user_database where user_id == User ID, which in this specific case requires that the user_id is a field in the document), you can dynamically search for documents that match your criteria. This approach is practical when the exact document path is unknown, but certain identifying features are available. In all cases you have to make sure that your App Engine or Cloud Run default service account has Cloud Datastore User permissions to read from Firestore.Once the correct document is found, the variable requires you to extract specific values (which can also be complex objects) by specifying a Key Path—like customer_score. In our case, the user’s customer_score value can be passed on as a user property to GA4 to unlock analysis use case or for audience building, which can then be shared within the Google Marketing Platform (GMP) for retargeting or look-alike audiences. But like with all variables, you can use the retrieved data in both tags and triggers of your sGTM setup to either fire tags conditionally or to enrich the data sent to all sorts of marketing vendors.Although not quite “god-like,” the Firestore Lookup variable offers an efficient tool for extending the power of sGTM with real-time data from Firestore, proving its worth as a critical component in the sGTM data integration stack.For those of you who are interested in a more detailed guide on how to set up the Firestore Lookup variable in sGTM, I recommend heading over to Simo Ahava’s extensive blog post.Artemis - The Huntress for DataJust as the goddess Artemis was known for her precision and skill in the hunt, the Artemis solution does incredibly well in tracking down and retrieving valuable data from Firestore.So, what makes Artemis the goddess different from the built-in Firestore Lookup variable, which I demoted to a demigod?Well, unlike the built-in Firestore Lookup variable, which focuses on fetching individual document attributes (e.g., the customer_score value), Artemis takes it a step further by fetching entire documents and allowing you to extract multiple values from a single API call. This makes the Artemis solution an efficient tool to optimize cost and performance by reducing the number of API calls needed to retrieve data from Firestore. So, if you need to get an entire Firestore document or plan to extract multiple values from a single document - for example, multiple user or product characteristics - Artemis is the goddess you should call upon.Artemis Variable Templates in GTM Server-SideFrom a technical perspective, the Artemis solution comes with two variable templates: The first is designed to fetch an entire Firestore document, while the second one allows you to extract specific values from that document using a Key Path.The first variable template uses the Firestore.read and the Promise APIs to asynchronously fetch data from Firestore. For the setup, all you have to do is provide the GCP project ID and enter the collection name and the document ID from which to retrieve the document. Eventually, the variable returns a stringified JSON object with the document’s data from a single API call. In my example, the variable Artemis - user_data variable would return the following JSON object for the document with the ID 6d193064-61ad-4f46-9780-7b6041899d7e:'{\"user_id\":\"6d193064-61ad-4f46-9780-7b6041899d7e\", \"customer_score\":\"gold\", \"order_count\":\"7\"}';The second variable template takes the stringified JSON object from a variable instance of the first variable template as an input and extracts a specific value using the provided key. This lets you pinpoint the needed data and pass it on to your tags or triggers in sGTM.In my example, the second variable template would extract the customer_score value from the stringified JSON object and return \"gold\".With Artemis, you can seamlessly integrate Firestore with sGTM to power tailored marketing strategies using first-party data stored in the cloud. Artemis allows you to hunt for data like its namesake, making it a valuable asset in your sGTM data integration stack.Soteria - The Safeguard of DataSoteria, the goddess of safety, deliverance, and protection, is for sure one of the lesser-known Greek deities. But, the Soteria solution shouldn’t suffer the same fate, as I see it as a crucial part of the GTM Server-Side Pantheon that can help you get more out of your marketing spend in GMP.Value-based bidding (VBB) is one of the most powerful tools in the GMP, as it allows you to optimize your bids based on the value that each conversion brings to your business. However, to use VBB effectively, you need to provide Google Ads with the conversion value for each conversion event. Obviously, the closer the conversion value is to the actual contribution to your business goals, the more you get out of marketing efforts.Traditionally, most companies calculate this value based on the revenue generated by the conversion event, but only in some cases is the revenue the relevant conversion value. Digitally mature businesses have realized that the actual conversion value can be more complex and might depend on various factors, such as the return rate, discounts, or profit margin.Before the advent of sGTM, it was pretty challenging to calculate the conversion value dynamically and send it to GMP in real-time. Hence, I’ve seen companies going down one of two paths: either they retrieved sensitive data like profit margins (e.g., by requesting that data through an open API) and exposed it to the dataLayer, or they reverted to adjusting the conversion value in batch using the GMP APIs. Both approaches have their downsides: the first exposes sensitive data to the browser, while the second is not real-time and might negatively impact bidding due to the delay in updating the value.Soteria Architecture and Data Flow diagramSoteria, the goddess of safety, solves this problem. She allows you to calculate the conversion value dynamically in sGTM and send it to GMP, GA4, or other vendors in real time without exposing sensitive data to the browser. Once again, the solution is based on the Firestore.read and Promise APIs and comes with its own variable template to fetch the necessary data from Firestore.The solution is designed to work out of the box with the incoming GA4 purchase event data, which contains the item_id for each product in the transaction. The variable looks up the profit value for each product and calculates the total profit-based conversion value. The newly computed conversion value is then passed to the Conversion Value or Revenue setting in the respective Google Ads or Floodlight Sales tags.Now, don’t all Google Ads and Floodlight tags triggered in sGTM result in a client-side request to eventually send the data to the platforms? How can you claim that the profit value (or other sensitive data) is not exposed in the user’s browser?Well, the answer is simple: Both tag types actually encrypt the conversion value before passing it back to the browser, so the data is never exposed to the user. This is a crucial point to understand, as it allows you to send sensitive data to GMP without compromising control over your data.Exemplary Firestore Database Setup for SoteriaThe technical requirements for the Firestore database are fairly straightforward. All you need is a collection with the item_id as the document ID and the profit value (or others like return_rate) as an attribute. The item_id is then used to look up the profit value for each product in the transaction.Let’s examine the actual variable template and its functionalities to “connect” it to the exemplary “items” database.Soteria Variable Template in GTM Server-SideAs you can see, the Soteria solution supports three different calculation methods:  Value  Return rate  Value with DiscountBased on the chosen method, the calculations include the profit value (Firestore), the return_rate (Firestore), the quantity of the product sold (retrieved from GA4 purchase event’s items array), and the discount. Under the hood, the following formulas are used to calculate the conversion value based on the selected method:conversion_value = profit * quantity; // \"Value\" settingconversion_value = (1 - return_rate) * profit * quantity; // \"Return Rate\" settingconversion_value = (profit - discount) * quantity; // \"Value with Discount\" settingIn case a product cannot be found in the database, the template allows you to specify three different fallback mechanisms using values from the original purchase event data:  Zero  Revenue  Percentconversion_value = 0; // \"Zero\" settingconversion_value = price * quantity; // \"Revenue\" settingconversion_value = price * percent * quantity; // \"Percent\" setting, with the variable being the custom \"FallBack Percentage\"If you haven’t already, you should definitely consider using Soteria or at least test it out—summoning her is well worth it!Based on numerous Google case studies and from experience with my own clients, I’ve seen it significantly improve the performance of their GMP campaigns, simply because the conversion value is now more closely aligned with the actual business value. As always, the results can be impressive when operating at the sweet spot of data, tech, and actual business use cases.Hephaestus - The God of Fire(store)As a smithing god, Hephaestus made all the weapons of the gods. He served as their blacksmith and was considered the god of craftsmen, metallurgy, and fire. Moreover, Hephaestus was the only Olympian god to do actual work, which is also fitting for the Hephaestus solution, as it allows you not only to lazily read data Firestore but also actively write to it.Hephaestus tag template settingsUnlike the previous solutions, Hephaestus comes in the form of a tag template. This template requires you to specify the GCP project ID, the collection name, the document ID, and the data you want to write to Firestore. The tag template uses the Firestore.read, Firestore.write, and the Promise APIs to asynchronously write data to Firestore, allowing you to store and update information in real-time.In the tag, you further specify which attributes (key-value pairs) you want to write to Firestore and whether you want to replace the existing document with the latest data or merge the new data with the existing one.Hephaestus requests for “Edit or add attributes” settingsIn the example above, which uses the “Edit or add attributes” setting, the tag would, therefore, first read the existing document from Firestore, then update the latest_transaction_id attribute with the value from the GA4 event data, and finally write the updated document back to Firestore. On the other hand, the “Replace entire document” setting would replace the entire document with the new data, which can be useful when storing only the latest user interaction data.Hephaestus can be a true asset if you want to maintain an up-to-date user profile in Firestore that can be edited in batch from the backend or in real-time with data from GTM. If this fits your use case, you could store the user’s last interaction with your website, the previous product they viewed, or the last page they visited and bring in fresh data from your CRM every night. Accessing an enriched user profile is valuable for all your personalization or targeting efforts and you can easily combine the Hephaestus solution with e.g. built-in Firestore lookups or Artemis.As I mentioned before, the principles used for these Google solutions have been introduced previously. Hence, if you are interested in this topic, I can also recommend checking out Stape’s Firestore Writer tag template that has similar functionality to Hephaestus but comes with more bells and whistles. For example, it allows you to write the entirety of the available event data to Firestore, can quickly add timestamps, etc. - and it’s also a great starting point for building custom tags.  Nice to know: I am currently working on a modified Hephaestus tag template that maintains a real-time count of, e.g., item purchases, event counts, etc., to build a simple analytics system where the server container writes the hit stream data to Firestore and a dashboard app hosted in Cloud Run reads and visualizes it. See the below video to get an idea of the putcome. Stay tuned for more on this!ConclusionYes, I know… This is another lengthy blog post, but I hope you enjoyed the journey through the Greek Pantheon of GTM Server-Side solutions. Midway through the post, I realized that cramping all the solutions into one post would be too much, so I split it into five parts. In this first part, we explored the possibilities of integrating Firestore into sGTM using the Artemis, Soteria, and Hephaestus solutions.We learned how to fetch entire documents, extract specific values, and write data to Firestore, enabling businesses to enrich their data streams and enhance their marketing strategies. In the next part of this series, I’ll discuss how to integrate BigQuery with sGTM.Even if the out-of-the-box templates outlined in this post do not meet your requirements 100%, they can be a great starting point for building custom tags or variables that fit your specific use case. The most important part I want to convey is how much more potential is in your data if you combine it cleverly. Firestore and sGTM are mere vehicles that allow you to do just that, giving you the power to rethink and shape your data strategy.If you are unsure about which use cases could help you meet your business goals, require more customized functionality, or want to implement one of the described approaches, feel free to contact me; I am always happy to help.I hope you enjoyed reading this blog post as much as I did writing it. If you have any questions or feedback, please get in touch with me on LinkedIn or via Email. Until next time, happy tracking!"
  },
  
  {
    "title": "Who are my website users? And if so how many? - User IDs in GA4",
    "url": "/gunnargriese.github.io/posts/ga4-reporting-identity/",
    "categories": "GA4",
    "tags": "ga4",
    "date": "2024-09-01 00:00:01 +0200",
    





    
    "snippet": "Google Analytics 4 (GA4) can unify your users’ journeys using various methods, like User ID, Device ID, and Modeling. These methods allow GA4 to create a single user journey from all the event data...",
    "content": "Google Analytics 4 (GA4) can unify your users’ journeys using various methods, like User ID, Device ID, and Modeling. These methods allow GA4 to create a single user journey from all the event data associated with the same identity, visualizing it in the interface without any further setup. This enables a more unified, holistic history of users’ interactions with your business and eventually allows you to report on user and session counts as well as associated metrics.This blog post provides an overview of the Reporting Identity functionality with a special focus on logged in users, its quirks in GA4, its usage for your business, and its implications for reporting. Eventually, I will try to help you answer the (somewhat philosophical) question: “Who are my website users? And if so how many?”What is the Reporting Identity in GA4?I know you’re eager to cut to the chase, and I hate to hold you off, but before we can dive deeper into the topic, we have to establish some common ground and get the definitions straight:  The identifiers GA4 uses to unify a user’s journey are collectively referred to as identity spaces.  The identity space(s) used by your GA4 property is called its reporting identity.In the following sections you’ll find a description of each of the available identity spaces and how they are used in GA4.Usage of identity spaces for Blended &amp; Observed reporting identityUser ID - Get to truly know your customersIf you create your own persistent IDs for signed-in users (e.g., CRM IDs), you can use these IDs to measure user journeys across devices and browsers. This requires consistently assigning IDs to your users and including the IDs along with the data you send to GA4 whenever they are browsing your website while logged in. The user ID is the most accurate and robust identity space because it uses the self-authenticated users’ ID that you then collect in GA4. Since these identifiers are the most stable and long-lasting identifiers and can especially be used for activation use cases, having a login functionality and nudging your users to use it is of great value for your business.  The user IDs that you establish must adhere to GA4’s Terms of Service. This necessitates that you transparently communicate to your users the manner in which identifiers are utilized, as outlined in your Privacy Policy. Furthermore, the ID you assign should not encompass information that could potentially enable a third party to ascertain the identity of an individual user, such as an email address.Google Signals - Google’s data about your usersGoogle Signals is data from users signed in to their Google account on their Chrome browser or Android device. When Google Signals data is available, GA4 associates event data it collects from users with the Google accounts of signed-in users who have consented to share this information (ads personalization consent is given in Google account settings).  Google stopped using Google Signals as an identity space in February 2024, as it came with the significant drawback that its usage resulted in data points being withheld from the GA4 user interface, reducing the tool’s value for actual analytics work. I have decided to include it here anyway because, in the past, it has been a heavily debated identity space.User-Provided Data - 3rd-party cookie alternativeOne of the most recently added functionalities in GA4 is “User-provided data collection”. This allows you to send “consented, first-party data from your website to Google Analytics” (s. here). Google allows for hashed email addresses, phone numbers, names, and addresses to be sent to their platform. The data you send is then matched with other Google data (primarily browser data from logged-in Chrome users) to improve the accuracy of your measurement data and power capabilities like Enhanced Conversions and Demographics and Interests. I like to think of this method as replacing Google Signals for when Google will eventually phase out 3rd-party cookies - but on steroids.Implementing the User-ID feature may not be possible for some websites (e.g., if your website doesn’t have a login section). But for other websites, especially e-commerce sites, Google recommends that you set up the User-ID feature along with user-provided data collection to provide the most accurate user reporting in Analytics.Interestingly enough, Google states in their documentation the following:  “If you send user-provided data without also sending user IDs, collected user-provided data will be pseudonymized and used to recognize unique users for user deduplication and reporting purposes. When multiple types of user-provided data are provided, Analytics will prioritize them in the following order: email, phone, name, and address. Note that if user IDs are later provided for previously measured users with only user-provided data (no user ID), Analytics will recognize these as separate users for reporting purposes.” (s. here)So, although it’s not part of the officially listed reporting identities, user-provided data appears to be used for user deduplication and reporting purposes, rendering it a de facto identity space. Since it’s quite a new feature, I am currently working together with my clients on understanding this feature even better, but have added it to this overview already.Device ID - Good ol’ cookiesGA4 can also use the device ID (also known as client ID or user pseudo ID) as an identity space. The device ID method is the least accurate because it only recognizes a device as the name suggests (not a user). At the same time, it is likely the most widely used method to identify users, as almost all analytics tools out there use it.  For apps: This is set to the App-Instance ID, which is unique to each app instance downloaded on a device.  For websites: This is set by first-party cookies (_ga cookie for JS-managed or FPID cookie (or another custom cookie name) for GTM Server Side-managed cookies). You can inspect a user’s cookie ID in the “User Explorer” report, but be aware that it is called “Effective user ID” regardless if it was collected from a website or mobile app.  If - for whatever reason - you want to rename the _ga cookie to something else, you can do so by adding a prefix using the cookie_prefix field in the GA4 configuration in GTM.Behavioural Modelling - The black boxWhen users decline GA4 identifiers like cookies, behavioural data for those users is unavailable. GA4 fills this gap by using the data of users who do accept cookies from the same property to model the behaviour of the users who decline cookies. For modelling to be working, the website needs to have Google Consent Mode implemented and the associated GA4 property needs to have sufficient data to be eligible for behavioural modelling.Reporting Identity Options in GA4In your GA4’s property settings, you decide which identity spaces you want to use. The options in your settings then determine which of the available identity spaces GA4 will take into account if you make the required data available to GA4. The setting options are:  Blended: Uses the User-ID, Device ID, then Modelling, in that order of preference.  Observed: It uses the User-ID, Google Signals, and then Device ID, ignoring Modelling.  Device-based: Only the device ID is used, and all other collected IDs are ignored.Adjusting the Reporting Identity for a GA4 property requires you to navigate to the Admin section, select the correct account and property, click on Reporting Identity in the Property column, select the Reporting Identity you want to use, and click Save.The reporting identity option you choose does not affect data collection or processing. Hence, you can switch between the options at any time without making any permanent impact on data. This is pretty neat, as you can directly assess within the interface what the impact on different identity spaces is on your data. Be aware that in order to get the full benefit of the Blended and Observed Reporting Identity, you need to collect user IDs.Available Reporting Identities in GA4Implementation of Identity SpacesSo far, so good. Now, let’s get a tad more hands-on and have a look at how to implement the different identity spaces to unlock them for your reporting in GA4.User ID - Take advantage of the dataLayerTo enable the User ID as an identity space for GA4, a user’s internal ID needs to be actively sent to the analytics platform. For the GA4 tags in GTM being able to pick up the user ID, your developers will usually expose the user ID to the dataLayer whenever a user initially logs in. A respective dataLayer object could be structured as follows and sent with the dataLayer.push() method:window.dataLayer.push({  event: \"login\",  // other event data  user: {    id: \"76588c85-b7a5-4967-9de7-695788e7a6f6\",    // other user data  },  status: \"success\",});From the moment on that a user has logged in, it is crucial that we enrich all GA4 events with the user’s ID. Therefore, we also need the developers to expose the ID on all of the following pages as early as possible, e.g., on page_view events.window.dataLayer.push({  event: \"page_view\",  // other event data  user: {    id: \"76588c85-b7a5-4967-9de7-695788e7a6f6\",    // other user data  },});If the value of the user ID key is populated, it can be read by a dataLayer variable in GTM.Exemplary GTM dataLayer variable for the user id keyThe value of the dataLayer variable is then included in all GA4 events as the value for the user_id field and sent with every outgoing GA4 request (via the uid query parameter).Exemplary GA4 settings variable for the user id keyThe variable above should then be associated with all GA4 tags. That way, we ensure that if the user ID is available in the dataLayer, we send it to GA4 - allowing for it to be used as an identity space.  Important: I advise you to actually use the user_id field in your GA4 tags, as it is the field that GA4 expects to receive the user ID in. If you use a user property or event parameter to store the user ID (e.g., for analysis purposes or custom joins) and register these as a custom dimension in the interface, you risk introducing a high-cardinality dimension that can lead to you seeing the (other) row in your reports.Device-based - Cookies for the massesAs mentioned above, cookies are the “default” method to identify users in analytics tools like GA4.In a standard client-side measurement scenario, firing a Google or GA4 tag will result in the _ga cookie being placed in the user’s browser via the JavaScript document.cookie API. The cookie’s value is what GA4 titles the user’s device ID. The gtag.js tracking library then reads and updates the cookie value as specified in the script and attaches the value to all outgoing network requests to the GA4 servers (cid query parameter). GA4 uses the device ID to connect the event data points into a coherent user profile on which you can eventually report.GA4 client settings to manage FPID cookiesIf you add GTM Server-Side (GTM SS) to your measurement stack, you can gain even more control over the cookie that determines your users’ Device IDs. In your GTM SS container, you’ll find that the standard GA4 client comes with additional configuration possibilities for Cookies and Client Identification. The client allows you to use your “own” first-party HTTP cookies (default name is FPID, but can be customized) to store the device ID. If you now route your GA4 requests through GTM SS and use the GA4 client to parse the event data, your GA4 tags will use the device ID stored in the FPID cookie instead of the one from the _ga cookie.In the current digital landscape, which includes browser tracking prevention and ad blockers, cookies are often considered one of the less reliable user identifiers. They are easily deleted or expired, leading to potential overcounting of “users” on our website. However, they remain the primary identifier for the majority of our website users. While we can exert more control over cookies, overcoming the limitations of the technology itself is a complex task. So, should you embark on this journey to improve the reliability of your device ID, make sure you have a good reason (and a business case) to do so.Modelling - Let Google handle itTo unlock the Modelling identity space you must leverage an Advanced Consent Mode implementation. In short, this means that you will execute your GA4 tags regardless of the users’ consent choices (e.g., no blocking of tags in case of missing consent). Additionally, you must ensure that you have Google’s Consent Mode correctly integrated with your website’s Cookie Management Platform (CMP) or in your app to control the tags’ (cookie setting) behavior based on the users’ consent decisions.Once Advanced Consent Mode is configured on your website, GA4 tags will function as usual for consented users. In contrast, in the case of missing consent, the GA4 tags will send only so-called cookieless pings to GA4 servers without placing cookies in the users’ browsers. Using machine learning, Google will derive patterns from the consented (or as Google calls it “observed”) users and apply them to the unconsented or unobserved users, modeling their user journeys and derived metrics. Google phrases the process as Behavioral Modeling, which eventually allows insights into your website users’ behavior and not only the consented subset.  One interesting side fact is that the User ID will be available in the BigQuery raw data export, even if the user denies cookie consent and GA4 collects only “cookieless pings”.Since Consent Mode and its implementation are vast subjects with quite a few nitty-gritty details, I’d like to refer you to my in-depth guide, which I published earlier this year. I’m sure it will serve you well in navigating that topic!Identity Space Implications on ReportingDepending on which Reporting Identity you choose in your GA4 settings, the data you see in the GA4 interface will be different. The selected Reporting Identity influences how GA4 associates events with users and sessions, affecting the way GA4 reports on user and session counts and associated metrics. In the following paragraphs, I will focus on the implications related to the User ID and Modelling identity space since these require special attention (and are somewhat out of the ordinary).User ID in the GA4 interfaceLet’s prioritize the User ID, as it is the most powerful identifier in GA4 and will be treated equally in this blog post. So, here it goes!Retroactive User ID AssignmentSuppose a user initially accesses a website without being logged in and logging in mid-session, transmitting a User ID during later events. In that case, GA4 uses the “User Pseudo ID” / client ID and the session ID to associate that session and all its events with the user ID provided when the user signs in.Retroactive User ID AttributionIn the example above, the User Explorer report will only display one “Effective User ID” (the User ID) set to abc-567 for both events if Blended or Observed are used as the Reporting Identity. Suppose the Device-based Reporting Identity has been chosen. In that case, the User Explorer report will only display one “Effective User ID” (the User Pseudo ID) set to 123.456 for both events. Furthermore, GA4 will report only one session for the two events, regardless of the Reporting Identity chosen.Retroactive User ID Assignment in BQ raw data, User Explorer, and Traffic Acquisition ReportThe screenshots above show how a mid-session login affects the raw data and the reporting in the User Interface (UI). While on the event level in the raw data, the user_id column is only populated from the time it’s been collected, we can see (e.g., based on the event count) that it’s being retroactively applied in the UI. Hence, it allows us to report on the “true” user journey and their associated sessions in the UI.I consider this a helpful feature of GA4, especially since it is seamlessly integrated into the UI and doesn’t require any additional effort to use.What about proactive User ID assignment?A limitation of the User ID is that if GA4 detects subsequent events without the User ID set for the same User Pseudo ID, these will not be assigned to the User ID. So, once a user signs out and the uid parameter is no longer associated with the events, GA4 stops associating subsequent events with that user ID. See the visual below for a better understanding.Retroactive User ID Assignment LimitationsIn the scenario above, the User Explorer report in GA4 will display two distinct Effective User IDs, signifying two users: 123.456 (cookie ID) and abc-567 (user ID).As we have seen earlier, the initial three events will be associated with the user_id. Event number 4 will then be assigned to the cookie ID 123.456 only—and not the user_id. Additionally, it will also come with its own session. This behavior might “inflate” the user and session count in your reports, as the same user is counted twice.This also means that it is of the utmost importance to ensure that the user_id field is populated when a deterministic identifier is available. Otherwise, your reports will contain fragmented user journeys and sessions.Distinguish between signed-in and non-signed-in usersWhen you enable the User ID feature, you can compare signed-in and non-signed-in users in GA4. This is especially useful when you want to understand the behavior of users signed in to your website or app versus those not signed in. Below you can see a comparison between signed-in and non-signed-in users in GA4 utilizing GA4’s Comparison feature in combination with the dimension Signed in with User ID. This allows us to easily distinguish between these two groups of users.Comparison between signed-in and non-signed-in users in GA4  (Please ignore the high Unassigned traffic share. I am still fine-tuning my synthetic user generator. 🧪🤖)Replication of User ID in BigQueryWhen you export your GA4 data to BigQuery, you will notice that both the user_id and the user_pseudo_id columns are available for analysis. Most likely, though, the user_id column will not be populated for most of your users. The user_id field is only populated when the User ID is collected via your on-site measurement. If the User ID is not collected, the user_id field will contain a NULL value, while GA4 will always populate the user_pseudo_id field with the Device ID. This is important to remember when you are querying your GA4 data in BigQuery and trying to match it to the interface.If you seek to replicate the interface’s reports in BigQuery, you can use the following query to calculate the effective user ID and session ID based on the user_id and user_pseudo_id fields. This query will allow you to see the same user journey in BigQuery as you would see in the GA4 interface - taking into account retroactive User ID association as outlined above.-- Add temporary function to easily access ga4EventParamsSELECT  event_timestamp,  event_name,  user_id,  user_pseudo_id,  COALESCE( FIRST_VALUE(user_id IGNORE NULLS) OVER (PARTITION BY CONCAT(user_pseudo_id, ga4EventParams('ga_session_id', event_params).value)    ORDER BY      event_timestamp ROWS BETWEEN CURRENT ROW      AND UNBOUNDED FOLLOWING), user_pseudo_id) AS effective_user_id,  ga4EventParams('ga_session_id',    event_params).value AS raw_session_id,  CONCAT( COALESCE( FIRST_VALUE(user_id IGNORE NULLS) OVER (PARTITION BY CONCAT(user_pseudo_id, ga4EventParams('ga_session_id', event_params).value)      ORDER BY        event_timestamp ROWS BETWEEN CURRENT ROW        AND UNBOUNDED FOLLOWING), user_pseudo_id ), ga4EventParams('ga_session_id',      event_params).value ) AS effective_session_idFROM  `&lt;your-project-id&gt;.analytics_&lt;your-property-id&gt;.events_*`WHERE  _TABLE_SUFFIX BETWEEN '&lt;your-start-date&gt;'  AND '&lt;your-end-date&gt;'  AND user_pseudo_id = '2030451950.1721832151' -- select a specific user for testingORDER BY  event_timestamp ASCThe effective_user_id is calculated using a window function that partitions the data by a combination of user_pseudo_id and session_id. The ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING ensures that the FIRST_VALUE window function captures the first non-null user_id from the current row onward within the partition. This logic stops populating effective_user_id with the user_id once the user logs out because, after the logout event, the subsequent rows no longer have a valid user_id within the remaining partition, causing the COALESCE function to revert to user_pseudo_id, effectively resetting the default cookie-based identifier for the rest of the session. You can inspect the result of the query in the screenshot below to see how the query effectively resolves the available identity spaces:Calculate effective user and session id in BigQuery for User IDs(Behavioral) Modeling in the GA4 interfaceWhen enabling the Behavioral Modeling identity space, GA4 will use their proprietary Machine Learning algorithm to model the behavior of users who have not consented to cookies (but for which you collect cookieless pings) based on the behavior of users who have consented to cookies. This means that the user journey data you see in the GA4 interface for these users is not the actual data but a model of the data. While the model is quite accurate from my experiences and tests, it is essential to remember that it is an estimate - not more, not less.Behavioral Modeling Data Quality Card in GA4Luckily, GA4 provides you with a data-quality icon (s. above) that indicates whether the data you are looking at is modeled and, if so, when the Modeling identity space was unlocked. This is especially helpful when you include date ranges in your analysis when the Modeling identity space has yet to be enabled. So, while it is a good feature to help you assess high-level trends, like overall user and session counts, you should always take the results with a grain of salt.Advanced Use Cases for User IDs in GA4As mentioned before, the User ID identity space is the most beneficial one. Not only because it is the most accurate and robust identity space but mainly because it sets you up with a solid foundation for more advanced use cases. Namely, enriching your website and app data with CRM or other data from other business-relevant applications and using it to build meaningful audiences. You can use the User ID to create more personalized experiences for users. For example, you can use the User ID to link a user’s behavior on your website or app to their profile in your CRM system, allowing you to tweak their experience based on their past behavior.Let’s have a look at the various methods for enriching first-party data in GA4.Using the GA4 Measurement ProtocolThe Measurement Protocol (MP) is a method for sending data to GA4 server-to-server. Accordingly, it allows you to send data to GA4 from any internet-connected device, such as a CRM. This method is especially useful for augmenting your GA4 data with data points unavailable client-side—exactly what we need for our User ID use case.To send data to GA4 using the MP, you need to send a POST request from our system to the GA4 endpoint with the required parameters. The following example shows how to send a user property along with an event:const measurementId = \"&lt;your-stream-id&gt;\";const apiSecret = \"&lt;your-api-secret-value&gt;\";const clientId = \"&lt;client-id&gt;\"; // Needs to match the client-side identifierconst userId = \"&lt;user-id&gt;\"; // Ensure userId is definedconst customerScore = getCustomerScore(userId); // Function to get user's customer scoreconst queryParams = `?measurement_id=${measurementId}&amp;api_secret=${apiSecret}`;const url = `https://www.google-analytics.com/mp/collect${queryParams}`;const payload = {  client_id: clientId, // The unique device identifier for a user.  user_id: userId, // The unique identifier for a user.  user_properties: {    customer_score: {      value: customerScore, // The user's customer score as a user property.    },  },  events: [    {      name: \"customer_score_enrichment\", // The event name.    },  ],};fetch(url, {  method: \"POST\",  headers: {    \"Content-Type\": \"application/json\",  },  body: JSON.stringify(payload),});In the snippet above, we “fetch” a customer score for a given User ID from our CRM system and send it to GA4 using the Measurement Protocol. The request includes the User ID, the user property customer_score, and an event named customer_score_enrichment.  Note: The sample above showcases how to send a user property along with an event, but sending a request that only contains user properties works just as fine.While the MP is a powerful tool, it can be challenging to fine-tune as it is easy to inflate the session count or fail to attribute events correctly to their session source. Hence, it requires a bit more technical know-how to set up and maintain—it’s a good idea to get technical support. If you want to learn more about the MP, I recommend checking out the official documentation.Using GTM Server-Side and FirestoreTo simplify the retrieval of first-party data, GTM Server-Side (GTM SS) comes with a Firestore Lookup variable, allowing you to pull values from specific keys or fields in a Firestore document to enrich data streams routed through GTM SS.GTM SS Firestore VariableIn the context of this blog post, we want to use the Firestore Lookup variable to enrich GA4 data with the customer score from the MP example. Our lookup key in this case is the User ID and we use the Firestore Lookup variable to retrieve a user’s respective score from a Firestore document and then send it to GA4 via a server-side event. This comes with the advantage that you can integrate the enrichment process directly into your existing measurement stack, making it easier to maintain and manage.  Note: Obviously, this requires that you have set up a Firestore database and populate it frequently with the users’ latest score values to ensure you feed GA4 with relevant data.Using GA4 Data ImportThe last method for data enrichment I want to introduce to you is GA4 Data Import. This feature allows you to perform a wide range of imports, such as cost data for your marketing campaigns, item data for your e-commerce events, offline events, and user data. The latter is especially interesting for us, as it allows us to import user data based on User IDs.GA4 Data Import InterfaceTo import user data based on User IDs, you must create a data set in GA4 and upload a CSV file containing the User IDs and the respective data you want to import. Alternatively, you can spin up an SFTP server and automate the upload of fresh import data that way.The data you upload will then be associated with the User IDs in GA4, allowing you to use it for analysis and audience building. For user data, the imported data is joined with the GA4 data upon collection/processing time, meaning that GA4 treats the imported data as if you collected it with the user’s event data from the beginning. The shortcoming of this approach is that the join doesn’t work retroactively, and GA4 will not join the imported with any of the historical data in your property.  Note: If you don’t use user IDs, GA4 also allows you to perform the import based on the device ID. This is especially handy for lead generation websites or websites that don’t have a login functionality.While the Data Import in its current state is a neat feature, adopting it could be more convenient. For example, manual uploads are suitable for one-off tests but otherwise impractical. The SFTP option comes with challenges and takes some time to set up correctly. I’m sure we’ll be able to use the Admin API to automate the import process in the future (like it was the case for Universal Analytics). Then, we could use serverless solutions (e.g., Cloud Run) to build simple, easy-to-maintain applications that automatically upload the data to GA4.If you feel like you need more information about GA4 Data Import based on User IDs, I recommend the official documentation to get started.ConclusionPhew—let’s take a deep breath together! That was a lot of information to digest, but I hope I conveyed it in a way that is easy to understand and follow. Furthermore, I hope you now better understand the Reporting Identity in GA4—especially the User ID—its intricacies and how to get started implementing the different identity spaces.Blending all the identities together into one aggregated overview can sometimes make it hard to get a clear picture of your users. However, understanding how the numbers you see in the GA4 interface come into existence is crucial before you report them to your stakeholders. This knowledge will keep you informed and confident in your reporting.Hopefully, I’ve managed to pique your interest in GA4’s User ID feature. This feature, along with a solid understanding of the GA4 interface, can unlock more advanced activation use cases for your business. If you’re eager to learn more about the role GA4 can play in your data activation efforts, I encourage you to explore my article GA4 - the CDP You Didn’t Know You Had for more insights and empowerment.If you have any questions or need further clarification, please don’t hesitate to contact me. I am always happy to help!"
  },
  
  {
    "title": "Ensuring Data Quality for GA4 at Scale with Google Cloud Platform",
    "url": "/gunnargriese.github.io/posts/ga4-data-quality-at-scale/",
    "categories": "GA4, BigQuery, GCP",
    "tags": "ga4, bigquery, sql",
    "date": "2024-07-06 00:00:01 +0200",
    





    
    "snippet": "  Disclaimer: This post of mine was first published on IIH Nordic’s website and has been slightly modified here. Head over there to read it in its original form.Businesses no longer collect clickst...",
    "content": "  Disclaimer: This post of mine was first published on IIH Nordic’s website and has been slightly modified here. Head over there to read it in its original form.Businesses no longer collect clickstream data with Google Analytics 4 (GA4) exclusively to enable data-informed decision-making by inspecting and analyzing the captured user data, assessing marketing campaign performance, or reporting on the most valuable landing pages. Companies that want to drive tangible business value use it as a data collection tool that feeds their marketing platforms with conversions and valuable audiences.This has always been the case, but with the advent of GTM Server-Side (sGTM) and its expanding capabilities, this integration becomes more profound.Source: Own Visualization of a sGTM setup powered by GA4As illustrated in the above visualization, businesses are migrating their data collection for services like Google Ads, Meta, Floodlight, and others from their client-side to server-side containers. This shift offers significant benefits, including additional data controls, optimized page load speed, and the option to enrich data streams with first-party data in real time. This enhanced data collection strategy opens up possibilities for custom solutions like real time dashboards and personalized communication.While the shift to GA4 and sGTM is a positive step that can potentially enhance digital marketing performance, it also brings a new challenge that has, so far, slipped the minds of those rushing to adopt this approach. The increasing number of vendors and tools relying on a single GA4 data stream highlights the critical importance of accurate data collection. Businesses must prioritize data quality to ensure the effectiveness of their digital marketing strategies.The traditional QA flow: A recipe for losing trustThe complexity and risks associated with the dominating approach become apparent when we examine the current state of data quality assurance (QA) within organizations.The traditional QA process for GA4 data collection, as well as measurement implementations for other marketing vendors, involves multiple departments and various layers – as you can see from the figure below. Usually, website developers and the measurement team collaborate on the dataLayer specifications and its implementation. The exposed events and associated values power the tag management system (e.g., Google Tag Manager), where the GA4 tags are configured to fire, read the desired values, and dispatch them to the GA4 servers. Eventually, the processed data is made available to a wide range of data consumers and decision-makers within the organization via the GA4 UI or dedicated dashboards.When no GA4 data quality measures are in place, the data recipients usually identify data inconsistencies and report their findings upstream to you and your colleagues in the measurement team. If you’ve ever gotten a call or an email pointing out discrepancies in your data collection while you were getting ready for the weekend, you can relate to how this can ruin your weekend plans (or at least I can). Once you receive this message, the work begins: Your team investigates whether they can confirm the issue or if the recipients lack context or training. If they confirm the issue, they need to identify the source—most probably by trying to replicate it. In the process, they need to communicate with the data consumers and web developers before the culprit is identified and eventually resolved.You can see how frustratingly cumbersome this process is. However, it’s still how most companies operate out there. With such a long QA flow, errors can spread quickly, impacting multiple areas before detection. Furthermore, the involvement of various departments increases response times, delaying error resolution. Inadequate tools often hinder efficient debugging at scale.The consequences of these shortcomings are severe:  Delayed Insights: Errors found by data consumers often mean that the data has been incorrect for a period, leading to delayed or misguided insights  Reduced Trust: Repeated errors discovered by end-users can erode trust in the analytics platform, making stakeholders less confident in the data  Increased Workload: Data consumers, tracking teams, and development teams all face increased workloads to identify, report, and fix issues, which can divert resources from more strategic initiatives  Operational Inefficiency: Finding and fixing errors post-collection leads to operational inefficienciesMoving from reactive to proactive data quality monitoringTo overcome the issues of this reactive approach, you, as a responsible member of your tracking team, should develop proactive measures that catch errors as early as possible before they manifest and are exposed as facts to your data consumers. Therefore, let’s explore some of the options we have at our disposal, spanning from built-in GA4 features to leveraging the BQ raw data exports to deploying a custom validation endpoint in the following section.Leveraging GA4 InsightsGA4 has a built-in insights feature that can automatically detect changes in your data. By setting up custom alerts (yes, letting go of good ol’ UA terminologies is hard), you can monitor essential data changes and receive email notifications when certain defined conditions are met. For example, you can create alerts for significant drops in active users or purchase events, ensuring timely interventions.Exemplary GA4 Insights and email notificationsThe configuration is simple and allows for quite some flexibility:  Evaluation frequency: Hourly (web-only), Daily, Weekly, Monthly  Segment: All Users is the default segment. Change to select other dimensions and dimension values. Specify whether to include or exclude the segment  Metric: Select the metric, condition, and value to set the threshold that triggers the insight. For example: 30-day active users – % decrease by more than – 20. If you choose Has anomaly for the condition, GA4 determines when the change in the metric is anomalous, and you do not need to enter a value.Having the ability to set up these notifications is a good step in the right direction, as it increases the chances that the measurement team catches errors in the data collection mechanism instead of putting this task on the data consumers. By moving the quality assurance closer to the source, we gain valuable time and control over addressing these issues (especially from a communication perspective to our end users).The custom insight feature is excellent for detecting unexpected fluctuations (=data volume) in relevant metrics, and most importantly, it’s free. Still, it falls short of addressing potential data quality issues. To do so, we would need much more granular configuration options to allow us to perform checks on event level.Stepping up our game with BigQuery and DataformI am not the first one to tell you this, but here it goes: Exporting your GA4 raw data to BQ will help you get the most out of your GA4 data. In this case, it’s our gateway to implementing powerful data quality checks for our data.GA4 BigQuery Export settings and schemaIntegrating BigQuery with your GA4 setup allows you to implement custom evaluation rules using SQL – limited only by your imagination. This integration turns BigQuery into a powerful tool for data quality monitoring.Using SQL in BigQuery, it is possible to develop custom rules to evaluate your data. For instance, you might set up rules to validate event data structures, ensuring they meet your predefined standards. To give you some inspiration:  Are all the expected ecommerce events being tracked?  Do all of these events have an item array associated with at least one item?  Do all items have an item ID, quantity, and value?  Do all purchase events have a transaction ID (in the expected format), and is the purchase revenue larger than 0?  Etc.It’s up to you to package your business logic into an SQL query that calculates the share of “falsy” events and identifies patterns as to why this is happening (e.g., specific page paths or browsers). Now, you might ask yourself: Do you really expect me to run such a query at the end of each day before I head home?No, not necessarily. You can use Dataform to further enhance this by allowing you to build and operationalize scalable data transformation pipelines. In particular, Dataform will enable you to alleviate the work of scheduling and evaluating the data quality query results. Dataform easily allows you to schedule queries and implement validations on top of query results using assertions.Exemplary architecture for integrating Dataform data quality checksFor example, in the below Dataform configuration, I am querying the page views from my blog where the custom dimension author is not populated. Furthermore, I’m specifying a rule that the share of these “bad” page views should not be more than 10% of all measured page views using Dataform’s assert function.Dataform configuration to identify bad page viewsAs many of you might be using Dataform already in their workflows to automate the creation of aggregated datasets for reporting, adding data quality assertions to the mix comes with minimal additional work but ensures you and your team deliver high-quality data to your end users. If you haven’t looked into Dataform yet, I highly recommend checking it out – don’t miss out on a huge time saver.Data Validation in RealtimeBut wouldn’t it be even better, if we could somehow move the data quality evaluation up to the point where the data actually originates from and thereby enable realtime monitoring for all the events we collect? If this were the case, we could act even faster on errors and fix bugs before multiple days of data are compromised. We could steer the communication around how to deal with these errors. We could even decide what to do with this erroneous data before it enters any downstream systems (- remember the first visual of this article?).You see, the benefits of such a real-time validation are tremendous, and it can enable a much more streamlined QA process for our GA4 data.The concepts of schemas and data contractsThe GA4 data collection for websites naturally originates from our users’ browsers when they visit our website. We then enable tracking for relevant user interactions via the website’s dataLayer (especially for ecommerce tracking) and use the dataLayer events to fire our GA4 event tags, which also pick up metadata according to the tags’ configurations. Eventually, the events and associated data are dispatched via requests directly to GA4 servers or our own sGTM container for further processing.Overview of GA4 data sourcesAs the overview above illustrates, all three GA4 data sources at the core consist of objects and key-value pairs that describe a given purchase event and its properties. Hence, we can think of every GA4 event as a JSON object that contains all necessary information about it for it to be processed in GA4.The above JSON contains information about a specific event but omits details that can lead to certain limitations when sending it to GA4. For instance, the above JSON objects might be:  Unclear: The examples don’t tell us which fields are required or optional, or what their respective types are. E.g., we do not know if the transaction ID field should always be a string or a number. Additionally, if it should be a string, we do not know its format. Should the transaction_id always start with a “T-” like in the example?  Incomplete: JSON objects lack complete data context, such as whether an item object should include an item_id or item_name field, and it does not indicate which fields can be omitted.  No Enforcement: JSON objects lack standardized validation and constraints, so it can’t enforce rules like an event requires a user_id if the login_status equals “loggedIn” or the item_category value being from a predefined list.For the standard GA4 events, Google provides us with extensive documentation of required key-value pairs and the values’ types. Furthermore, if you’ve been serious about your data collection in the past, you can rest assured that you have documentation about custom events and associated parameters as well.The need to validate an instance of a JSON object against a predefined set of rules that this object should adhere to is luckily nothing new in programming and has been solved already. One of the most effective ways to ensure data consistency and validity is through JSON Schemas. JSON Schema is a blueprint for JSON data that solves all of the issues listed. It defines the rules, structure, and constraints that the data should follow.JSON Schema uses a separate JSON document to provide the JSON data’s blueprint, which means the schema itself is also machine and human-readable. Or to rephrase it, we are using JSON to describe JSON.Let’s take a look at what the schema for our example dataLayer purchase event above could look like:Exemplary schema definition for a purchase dataLayer eventAs you can see, the schema above provides a lot more context to our initial purchase dataLayer object, among others:  Allowed values  Pattern constraints (RegEx validation)  List validation (e.g., minimum amount of objects in a list)  Key validation (e.g., must have key-value pairs in an object)This schema then functions as a sort of data contract that every GA4 event or dataLayer object needs to adhere to in order to be considered valid.Monitoring a website’s dataLayerWith JSON schema in our toolbox and the full power of GCP at our disposal, we can now put all the pieces together by building a lightweight validation endpoint that is capable of receiving dataLayer event objects from the website and validating them against the schemas defined by you.Exemplary architecture for dataLayer schema validationThis requires a custom HTML tag in the website’s container that reads the desired dataLayer object and sends it as the payload to our validation application that could be hosted on Cloud Run, App Engine, or Cloud Functions. The application would read the schema definitions and compare these against the received dataLayer events. The results of this validation (e.g., valid or not, potential error messages, etc.) would be written to BigQuery or the logs of the application.The resulting BigQuery tables enable a data quality cockpit that can be shared among all stakeholders – especially the measurement team and web developers responsible for the dataLayer implementation. The centralized dashboard can provide an overview of your data quality status, alerting you to potential issues and allowing you to take proactive measures.Monitoring GA4 eventsIf we are using sGTM for our GA4 data collection, we can integrate our custom validation endpoint with the container by using an asynchronous custom variable that forwards the event data object parsed by the GA4 client to the validator endpoint. The service will then respond with the validation result like below:Exemplary variable response that failed validationHaving this information available before any data is made available to GA4 or any other downstream vendors like Google Ads, Meta, or others allows for full control over how to treat events that are compromised:  Should these events be dropped altogether?  Should they be routed into a separate GA4 property for further investigation?  Should compromised events be used to trigger marketing tags?  Should the GA4 events simply be enriched with a data quality parameter?Exemplary architecture for GA4 data validationWhile the previous solution is great to monitor your data source and get notified when things break, using a direct integration with sGTM and enriching the GA4 data stream in realtime actually allows for enforcement of the data contract.Benefits of proactive data quality monitoringNo matter what option you go for, proactive data quality right at the source of the data monitoring offers several significant benefits, ensuring that your analytics data remains accurate, reliable, and actionable.QA flow focused on proactive measures  Increased Trust in Data: When data is consistently accurate and reliable, stakeholders develop greater confidence in the analytics platform. This trust is crucial for making informed business decisions and strategizing effectively  Operational Efficiency: By catching errors early in the data pipeline, proactive monitoring reduces the need for extensive post-collection data cleansing and correction. This efficiency saves time and resources, allowing teams to focus on more strategic initiatives rather than firefighting data issues  Cost Savings: Identifying and fixing data quality issues early in the process is generally less costly than addressing them after they have affected downstream systems and reports. Proactive monitoring helps avoid the financial impact of poor data quality on business operations  Improved Decision-Making: High-quality data leads to better analytics and insights, which are critical for making sound business decisions. Proactive monitoring ensures that decision-makers have access to accurate and timely information, reducing the risk of making choices based on faulty dataConclusionProactive data quality monitoring is not just a best practice; it is a necessity in the modern data landscape. By implementing robust monitoring and validation systems for their behavioral data collection in GA4, organizations can ensure the integrity of their data, build stakeholder trust, and maintain operational efficiency. The transition from reactive to proactive monitoring offers a strategic advantage, turning data quality management into a competitive differentiator.Investing in proactive monitoring not only safeguards your data but also enhances your organization’s ability to make timely, informed, and impactful decisions.For further assistance or to discuss how we can help you ensure data quality at scale, feel free to connect with me. I am always happy to help you on your data quality journey."
  },
  
  {
    "title": "Mapping GA4 Conversions in BigQuery for Comprehensive Dashboarding",
    "url": "/gunnargriese.github.io/posts/ga4-conversions-to-bigquery/",
    "categories": "GA4, BigQuery",
    "tags": "ga4, bigquery, sql",
    "date": "2024-02-07 23:00:01 +0100",
    





    
    "snippet": "Recently, I found myself in a situation where I needed to use Google Analytics (GA4) conversions in BigQuery (BQ) for a dashboarding project. The dashboard was built on raw GA4 event data extracted...",
    "content": "Recently, I found myself in a situation where I needed to use Google Analytics (GA4) conversions in BigQuery (BQ) for a dashboarding project. The dashboard was built on raw GA4 event data extracted from BQ, including data from multiple GA4 properties, each with its own set of conversion events managed by different teams and edited frequently.The challenge was that the GA4 raw data in BQ does not contain this information, and I had to find a way to get it there. In this post, I will show you different ways to achieve this - including my favorite one.The ProblemAs stated above, the events in the GA4 raw data in BQ do not have an event parameter indicating whether or not they are conversion events. You don’t believe me? Go ahead, feel free to pull your GA4 data from BQ and check for yourself…Now that you’re back, you can see that this means the daily raw data exports on its own are insufficient to calculate the number of conversions or a conversion rate for a given property, which is an everyday use case for many analysts and marketers (especially for reporting).Conversion Settings - Own GA4 PropertyThe conversion events are defined in the GA4 property settings in the UI and can be adjusted by any team member with Edit access to the property at any time. Here is the only place where the information of which event is a conversion is maintained. While this isn’t a big deal when you’re only dealing with a single GA4 property, keeping your SQL queries and the GA4 settings in sync becomes a real challenge when handling multiple GA4 properties that are managed independently but require some unified reporting.The Solution SpaceAfter some research, I came up with three different ways to get the information about conversion events into BQ:Solution Space Own Visualization  Manual Approach in SQL: Manually updating a table or the SQL query in BQ with the conversion events and their properties  Dynamic Approach Using GTM SS: Using Transformations in Google Tag Manager Server-Side (GTMSS) to add a custom event parameter to the GA4 events indicating whether or not they are conversion events  Dynamic Approach Using GA4 Admin API and GCP: Using the GA4 Admin API to get the conversion events and then uploading them to BQLet’s look at these approaches in detail to see how they would work.Manual Approach in SQLEmbedding the conversion events in the SQL query is the most straightforward way to get the conversion events into BQ. This approach could be used for properties with infrequently changing conversion events. The actual embedding can be done in many different ways - for example, by using a WITH clause, CASE ... WHEN... statements or by creating a separate table in BQ.In the example below I’m using a list of conversions events and evaluate the actual events against the list. If the event is in the list, it’s a conversion event and increases the total_conversions count by 1.SELECT  COUNT(DISTINCT user_pseudo_id) AS total_users,  COUNTIF(event_name IN ('click',      'new_desktop_user',      'purchase')) AS total_conversions, -- list of conversion events  COUNT(*) AS total_eventsFROM  `&lt;your-project-id&gt;.analytics_&lt;your-property-id&gt;.events_*`WHERE  _TABLE_SUFFIX BETWEEN '20240203'  AND '20240203';Although this approach appears to be simple to execute at first glance, the obvious downside of it is that you have to update the SQL query every time a conversion event is added, removed or changed. In practice this means that before you can run your SQL query, you have to check if the conversion events have changed and update the SQL query accordingly. This is not only time-consuming but also error-prone. Hence, I cannot recommend this approach in a production scenario.Dynamic Approach Using GTM SSA better way to do this is to use Google Tag Manager (GTM SS) to add a custom event parameter to the GA4 events, indicating whether or not they are conversion events. In this case, we use GA4’s tracking library (gtag.js) to flag conversion events with the query parameter &amp;_c=1 automatically. This parameter is present only for conversion events. It is furthermore exposed in GTM SS’s event data in the x-ga-systems_properties and will be true for any conversion event. We can access this parameter in GTM SS using a custom event data variable, like so:Exemplary Event Data Variable - Own GTM SS SetupWe can then enrich the GA4 tag in GTM SS with a custom event parameter indicating whether or not the event is a conversion event, like so:Exemplary Enriched GA4 Event Tag - Own GTM SS SetupFor our SQL queries we can incorporate this custom event parameter by e.g. filtering for events where the custom event parameter is true.The ability to enrich, redact, and modify the GA4 events in GTM SS amazes me every time I use it. It’s such a powerful tool, especially since we lost Universal Analytics customTask feature with the advent of GA4 and the ability to modify GA4 events in the browser before they are sent. GTM SS is a great way to fill this gap.The proposed setup doesn’t require much work or maintenance, and it ensures that whatever conversions are configured in the GA4 UI will be directly available in BQ. This approach even takes care of the variable time component of conversion specifications, as the events will only be flagged as conversions if they are configured as such at the time of the event occurence. Although this approach requires GTM SS, but if you’re already using GTM SS, this is a no-brainer. Setting up GTM SS for this purpose might be a massive overkill, though.Dynamic Approach Using GA4 Admin API and GCPTherefore, I developed another dynamic approach. Here, I use the GA4 Admin API and two more Google Cloud Platform (GCP) services to get the conversion metadata into BQ. It involves using the GA4 Admin API to fetch the conversion events and their properties and then uploading them to BQ via Cloud Functions, which can be repeatedly triggered with Cloud Scheduler. While the initial setup is a tad more complex than the previous approach, it’s the most flexible and versatile I could come up with.  Info: The GA4 Admin API allows you to manage GA4 properties and their resources programmatically and Google is still adding new features to it. The API is free to use and has a generous quota, that should be sufficient for most use cases.The Setup in GCPFor starters, we configure a Cloud Function that utilizes the GA4 Admin API’s properties.conversionEvents.list() method to fetch the conversion metadata for a given GA4 property:def get_ga4_conversions(property_id, conversion_results):    today = datetime.date.today().strftime('%Y-%m-%d')    ga4_resp = ga4_client.list_conversion_events(parent=f\"properties/{property_id}\")    for conversion_event in ga4_resp:        conversion_results.append(            {                \"date\": today,                \"property_id\": property_id,                \"conversion_api_name\": conversion_event.name,                \"event_name\": conversion_event.event_name,                \"create_time\": conversion_event.create_time.isoformat(),                \"counting_method\": conversion_event.counting_method.name,            }        )    return conversion_results  Info: Cloud Functions are a serverless execution environment for building and connecting cloud services. With Cloud Functions, you write simple, single-purpose functions that are attached to events emitted from your cloud infrastructure and services. Your Cloud Function is triggered when an event being watched is fired. Your code executes in a fully managed environment. There is no need to provision any infrastructure or worry about managing any servers.The Cloud Function then formats the conversion metadata into a JSON format that we see fit and uploads it to BQ using the bigquery.Client.load_table_from_json method:def bq_upload(data):    success = False    # Check for table and create if it doesn't exist    print(data)    try:        table = bq_client.get_table(TABLE_ID)    except:        table = bigquery.Table(TABLE_ID, schema=bq_schema)        bq_client.create_table(table)        time.sleep(10)    # Upload data    job_config = bigquery.LoadJobConfig(        schema=bq_schema,        write_disposition=\"WRITE_APPEND\",    )    job = bq_client.load_table_from_json(data, TABLE_ID, job_config=job_config)    job.result()    print(\"Uploaded {} rows to {}.\".format(job.output_rows, TABLE_ID))    success = True    return successThe resulting dataset will contain the following columns:  date(DATE) - The date of the Cloud Function execution.  property_id(STRING) - The GA4 property ID.  conversion_api_name(STRING) - The API name of the conversion.  event_name(STRING) - The event name of the conversion.  custom_event(BOOL) - Whether the conversion is a custom event (e.g., for purchase it’s false).  deletable(BOOL) - Whether the conversion is deletable.  create_time(TIMESTAMP) - The time the conversion was created.  counting_method(STRING) - The counting method of the conversion.  default_conversion_value(FLOAT) - The default conversion value.  default_conversion_value_currency_code(STRING) - The currency code of the default conversion value.  Info: BigQuery is a fully managed, serverless data warehouse that enables scalable analysis over petabytes of data, which supports querying using SQL. BQ is a powerful tool for data analysis and reporting.You can find the full code for the Cloud Function here as well as detailed instructions on how to set it up in your environment. The Cloud Function can be triggered by a time-driven trigger to fetch the conversion events and upload them to BQ. I chose to use GCP’s Cloud Scheduler to execute the Cloud Function in regular intervals.Cloud Scheduler - Example Setup  Info: Cloud Scheduler is a fully managed enterprise-grade cron job scheduler. It allows you to schedule virtually any cloud infrastructure operation. You can use it to automate your infrastructure, saving time, reducing human error, and reducing costs.That way, we can always ensure that we have the most recent conversion metadata in BQ. It is worth noting that the code above will append the recent conversion data to the table each time it is run. That way, we can keep track of the conversion metadata over time and even adjust our SQL queries to use it only for the date range for which it is valid.The cost implications of this approach are minimal. The Cloud Function is executed only when triggered by the Cloud Scheduler, and the GA4 Admin API is free. The only cost to consider is the cost of storing the conversion metadata in BQ, which is negligible.  DISCLAIMER: A similar mechanism could be built using Google Apps Script together with Google Sheets. In that case, the App Script takes on the role of the Cloud Functions and would be triggered by a time-driven trigger to fetch the conversion events from the GA4 Admin API and upload them to the connected Google Sheet. I prefer the Python approach because it’s more flexible, but if you need some (non-technical) user input, e.g., to add or remove conversion events, the Google Apps Script approach might be the better choice.Usage of Conversion Metadata in BigQueryNow that we have the conversion metadata in BQ, we can join the GA4 raw data with the conversion events. Given the rich metadata we exported before, we can now incorporate a lot of different metrics into our SQL queries (e.g., conversion value, eligible period, etc.). All of these are helpful to build reports similar to the GA4 UI.One of the most relevant data points obtained from the API is probably the respective counting method for a given conversion event. The counting method can be ONCE_PER_EVENT or ONCE_PER_SESSION and specifies if each time it occurs or is counted only once per session (like Universal Analytics goals). This differentiation should obviously be represented in the SQL query to give results matching the UI. Here’s an example of how to do this using the newly obtained conversion metadata:CREATE TEMP FUNCTION ga4EventParams(-- s. full code here: https://zielinsky.alejand.ro/using-ga4-event-parameters-as-a-custom-dimensions-in-bigquery-f60978527fd));WITH conversions AS ( --    SELECT        DISTINCT event_name,        counting_method    FROM        `nlp-api-test-260216.analytics_conversions.ga4_conversions`    WHERE        property_id = '250400352'),session_info AS ( -- calculate session-level conversions    SELECT        user_pseudo_id,        ga4EventParams('ga_session_id', event_params).value AS session_id,        EXTRACT(            DATE            FROM                TIMESTAMP_MICROS(event_timestamp) AT TIME ZONE \"Europe/Copenhagen\"        ) AS day,        COUNTIF(event_name IN (SELECT event_name FROM conversions WHERE counting_method = 'ONCE_PER_SESSION')) &gt; 0 AS has_session_conversion    FROM        `nlp-api-test-260216.analytics_250400352.events_20240204`    GROUP BY        user_pseudo_id,        session_id,        day)SELECT    day,    COUNT(DISTINCT s.user_pseudo_id) AS users,    COUNT(DISTINCT CONCAT(s.user_pseudo_id, session_id)) AS sessions,    COUNTIF(event_name = 'page_view') AS page_views,    COUNTIF(event_name IN (SELECT event_name FROM conversions WHERE counting_method = 'ONCE_PER_EVENT')) AS conversions, -- calculate event-level conversions    SUM(IF(has_session_conversion, 1, 0)) AS session_conversions,    COUNT(*) AS total_eventsFROM    session_info as s -- join event- and session-level conversions    JOIN `nlp-api-test-260216.analytics_250400352.events_20240204` as e ON CONCAT(s.user_pseudo_id, session_id) = CONCAT(e.user_pseudo_id, ga4EventParams('ga_session_id', event_params).value)GROUP BY    dayORDER BY    day ASC;The query above could be extended to include multiple properties, eligible periods for conversions, and conversion values. There’s plenty of possibilities here. If you come up with something cool, let me know!While this last approach requires more development effort and maintenance than the previous ones, the benefits are high. With the GA4 Admin API and GCP, we get the complete context of the conversion events and can use this information to build comprehensive reports in BQ, which we can expose to our dashboarding tool of choice.You can find the complete code as well as detailed deployment instructions for the GCP infrastructure here.ConclusionThe need for these workarounds stems from many companies trying to replicate the GA4 UI in BQ. At the same time, the BQ schema is not designed to be a 1:1 copy of the GA4 UI. Luckily, with the API-first approach of GA4, GTM SS’s flexibility to easily manipulate GA4 events in real-time, or the (almost) unlimited possibilities of GCP, we can build a comprehensive reporting environment in BQ that is tailored to our needs.To summarize the reviewed approaches, the manual approach is the most straightforward way to get the conversion events into BQ. However, there are better choices for fast-changing environments where the analyst does not fully control the data collection. The second approach, using GTM SS, is a good way to get the conversion events into BQ if you already use GTM SS. However, setting up GTM SS for this purpose is too much.The GA4 Admin API and GCP approach is the best way to get the conversion events into BQ. This approach is the most flexible. It’s also the most reliable way to get the conversion events into BQ, but it comes at the cost of some development effort and maintenance (which should be minimal).I hope this post was helpful to you. If you have any questions or feedback, feel free to contact me."
  },
  
  {
    "title": "Consent Mode v2 - A Comprehensive Technical Guide",
    "url": "/gunnargriese.github.io/posts/consent-mode-v2/",
    "categories": "GA4",
    "tags": "ga4, gtm, gtm-server-side, consent-mode, firebase-analytics",
    "date": "2024-01-30 23:00:01 +0100",
    





    
    "snippet": "In fall 2023, the EU has deemed Google as a gatekeeper in the digital space under the Digital Markets Act (DMA), placing it under heightened legal scrutiny. In response to this assessment and to me...",
    "content": "In fall 2023, the EU has deemed Google as a gatekeeper in the digital space under the Digital Markets Act (DMA), placing it under heightened legal scrutiny. In response to this assessment and to meet the DMA enforcement deadline in March 2024, Google is adjusting its advertising solutions. One aspect of this change is the latest update to Google Consent Mode. Starting in March 2024, Consent Mode will become mandatory for all advertisers operating in the EEA and wanting to use all of Google’s GMP advertising features. This shift, which includes phasing out certain features in the remaining UA360 properties while at the same time providing insufficient documentation on the new feature’s functionality and behavior, left the community with a lot of open questions.This blog post is the product of me penning down my observations and combining them with the ones from my peers in the industry to contribute to a better understanding of Google’s Consent Mode v2 and its impact on data collection. Overall, the aim is to provide readers (and me as the author) with a comprehensive overview of Consent Mode v2 and help them make informed decisions about their website’s privacy and data practices by unveiling the technical intricacies.This blog post can be seen as the technical counterpart of my previous thoughts on Consent Mode v2 published on the IIH Nordic blog. If you are new to the topic, I recommend reading the previous blog post first to get a better understanding of the topic’s context and the implications of Consent Mode v2.Introduction to Consent Mode v2Google’s Consent Mode v2 becomes mandatory on March 6, 2024, for all advertisers targeting the European Economic Area (EEA), including the UK. Implementing this update is essential for maintaining access to the Google Marketing Platform’s (GMP) remarketing and targeting functionalities, regardless of the company’s location, as long as it engages EEA users. The general idea of Consent Mode is to signal user consent states (granted vs. denied) for various consent types to Google tools and generate data through additional signals (so-called pings) even in the absence of user consent.The following consent types are supported by Consent Mode v2:            Consent Type      Description                  ad_storage      Enables storage, such as cookies (web) or device identifiers (apps), related to advertising              ad_user_data      New in v2: Sets consent for sending user data to Google for online advertising purposes.(Will be inferred from ad_storage )              ad_personalization      New in v2: Sets consent for personalized advertising              analytics_storage      Enables storage, such as cookies (web) or device identifiers (apps), related to analytics, for example, visit duration              functionality_storage      Enables storage that supports the functionality of the website or app, for example, language settings              personalization_storage      Enables storage related to personalization, for example, video recommendations              security_storage      Enables storage related to security such as authentication functionality, fraud prevention, and other user protection      The collected signals and additional data points aid in modeling conversions (e.g., for Google Ads and Floodlight) and tracking and modeling user behavior and conversions (for GA4). Consent Mode allows for (limited) data capture in line with user consent preferences through two variants: ‘Basic’ and ‘Advanced.’ The ‘Basic’ variant is the future minimum requirement for advertisers, whereas the ‘Advanced’ mode allows for fuller use of Google’s data modeling capabilities. The ‘Advanced’ variant enables data collection on websites through more or less complete requests, but without cookies, whenever consent is not given or explicitly denied. The aim is to fill data collection gaps and to provide advertisers with a more complete, modeled picture of their users’ behavior. These two options have been available since the initial release of Consent Mode. However, with the latest update, Google increased the focus on their availability.Source: Own VisualizationMoreover, with Consent Mode v2, two new consent states have been introduced: ad_user_data and ad_personalization. These two new consent states allow for more granular control over what data is collected and how the data is processed and shared between interconnected GMP tools, like GA, Google Ads, etc. The new consent states highly aim to control the data activation processes. In that sense, Consent Mode v2 increasingly becomes a control layer for advertisers for all data collection (ad_storage &amp; analytics_storage) and activation mechanisms (ad_user_data &amp; ad_personalization) in the GMP. This is an aspect that is often overlooked when discussing Consent Mode v2. Traditionally, the focus is very much on the data collection aspect for websites, but Consent Mode v2 is much more than that.Basic ImplementationWhen a user lands on a website or uses an app for the first time, the advertiser blocks Google tags and SDKs by default. At this point, the advertiser shares no user data with Google. As we know, the page then presents the user with a consent banner. The user can grant or deny tracking consent (if the setup is correctly implemented…).Source: Own Visualization of Google sourceShould the user withhold consent, no tracking occurs, no tags are fired, and Google does not receive any data from the user.Conversely, if the user agrees and provides consent, the advertiser updates the consent states for Consent Mode and conveys it to the Google tags/SDK. This action effectively lifts the blockade, allowing the tags and SDKs to function as intended (=using cookies).This is most likely the implementation that most advertisers have implemented, as it is the easiest and least “risky” from a legal perspective. However, it also comes with the most significant drawback, as it only allows for data collection in the presence of consent. Advertisers that do not collect data for unconsented users will naturally experience a less complete picture of their user’s behavior.Advanced ImplementationAdvanced Consent Mode operates under a paradigm where advertisers do not entirely block Google tags and SDKs without user consent. Unlike the Basic Implementation, where consent is a prerequisite for data processing, the Advanced Mode proceeds with data collection regardless of consent. Still, it does not use any cookies in that case.When users deny consent, the Advanced Mode employs “cookieless pings”. These are regular events/hits sent to Google’s servers but do not incorporate regular cookie values. Cookieless pings then allow Google to use these for modeling granular and aggregated data for analytics (Behavioral Modeling) and advertisement performance purposes (Conversion Modeling).Source: Own Visualization of Google sourceThe advertiser communicates their users’ consent states to Google, which determines how Google processes, exposes and uses the data. From the table above, the reader can infer the exact implication of each consent state on the tags’ data collection behavior.The general idea of the Advanced Consent Mode is that even when cookies are not permissible, advertisers can obtain valuable insights in a privacy-conscious manner.Basic vs. Advanced ImplementationTo summarize, the Basic Implementation ensures that no data is collected without consent and utilizes cookies once consent is granted. The Advanced Implementation, on the other hand, additionally allows for data collection without consent, but without cookies in that case. Advanced Implementation is the only way to collect data that feeds Google’s embedded models without consent.Here are the tradeoffs between advanced and basic implementation for consent mode:                   Advanced Implementation      Basic Implementation                  Tag Behavior      Google tags are loaded before the consent dialog appears.  Tags send cookieless pings when cookie consent is declined.      Google tags are blocked until consent is granted.              Behavioral Modeling in GA4      yes      no              Conversion Modeling in GA4      yes      yes*              Conversion Modeling in Google Ads      yes      yes*      * When tags are blocked due to consent choices, no data is collected, and conversion modeling in Ads is based on a general model. The modes use features such as browser type, conversion action type, time of day, and other high-level, non-identofying variables. This model is less accurate than the model used when tags are unblocked, but it is more accurate than not using a model at all.Source: Advanced vs. basic implementation  DISCLAIMER: While advertisers should adopt the Consent Mode update to ensure remarketing efforts stay efficient, they should not take the choice of whether or not to implement the “Advanced” variant lightly. The “Advanced” mode allows for more data collection, which may benefit campaign performance. However, it might also come with a higher risk of non-compliance with legislation like ePrivacy, which could result in fines and other penalties. Hence, I strongly advocate involving your legal department in deciding on the abovementioned trade-off.Implications of Adopting Consent Mode v2With the two additional consent states — ad_user_data and ad_personalization — Consent Mode v2 allows for enhanced user consent granularity and gives advertisers tools to directly affect how Google services can utilize collected data.The ad_user_data consent state governs whether advertisers can send user data to Google for advertising purposes. Denial of this consent has the following implications:  User ID Collection: If ad_user_data consent is denied, no user-specific identifiers are collected or processed. This restriction negatively impacts the ability of advertisers to measure cross-device user interactions.  Enhanced Conversions: Denying consent for ad_user_data means that Enhanced Conversions, which rely on (hashed) first-party data (1PD), will not be processed in Google Ads and GA4. This limitation hinders the ability to track users’ post-ad interactions, potentially leading to less efficient ad performance measurement and ad spending.  GA4 Conversion Export: Additionally, the export of GA4 conversions to the GMP will cease to function without ad_user_data consent, disrupting the flow of conversion data used for broader marketing analysis and campaign adjustments.The ad_personalization consent state, on the other hand, controls the allowance for personalized advertising:  Remarketing Capabilities: With ad_personalization set to denied, the user will not enter any potential remarketing lists.  GMP Personalized Advertising: Using GMP tools for personalized advertising also becomes impossible without consent for ad_personalization. This restriction can significantly reduce the effectiveness of ad campaigns targeted based on user preferences and behaviors.The above functionality also explains why Google explicitly added these two new consent states to the remaining data collection mechanisms - APIs and Measurement Protocol. Advertisers can use these methods to enable remarketing use cases (e.g., Customer Match via API upload) or send offline conversions with 1PD (e.g., Enhanced Conversions for Leads). Hence, advertisers must also send users’ consent signals for these data points to ensure a comprehensively designed system from Google’s perspective.Implementation of Consent Mode v2For those who have previously integrated the previous version of Consent Mode and consciously decided whether to use Basic or Advanced, transitioning to v2 is a straightforward process that involves updating the existing configurations to recognize and handle the two new consent states.At the core of Consent Mode are two key commands that govern user consent states:  Default Command: This sets the initial consent state, which comes into play before a user has made their consent choices known. Establishing this state immediately upon every page load is the important thing here.  Update Command: This command is used to revise the consent state once the user has made their choices clear. It ensures the user’s consent preferences are captured and applied consistently to all applicable tags throughout browsing.To accurately implement these commands, it’s best to adhere to the protocols provided by your Consent Management Platform (CMP). Many CMPs offer step-by-step guides for integrating Consent Mode v2, which typically involve gtag.js and Google Tag Manager (GTM) Sandbox APIs, preferably via template configurations. Advertisers should consult their CMP’s documentation to ensure a smooth integration process.Using gtag.js (web)The Google tag (gtag.js) is a JavaScript tagging framework and API that allows you to send event data to Google Analytics, Google Ads, and other GMP tools. The gtag.js tagging library uses the global gtag() function to send data to Google’s servers. The gtag.js library supports Consent Mode v2 natively and allows for implementing the two key commands mentioned above.Set default consent stateIdeally, the default consent state is called on every page on your site and set before any other consent mode commands (or other Google tag execution). This ensures that the default consent state is applied to all subsequent gtag.js commands. Furthermore, the default consent state is applied to all subsequent gtag.js commands until the consent state is updated. The consent state values in this article are only examples and should match your organization’s policy for your specific implementation.window.gtag(\"consent\", \"default\", {  analytics_storage: \"denied\",  ad_storage: \"denied\",  ad_user_data: \"denied\",  ad_personalization: \"denied\",  wait_for_update: 500, // Optional. A millisecond value to control how long to wait before data is sent by any Google tags  region: [\"ES\", \"DE\"], // Optional. A region value to fine-tune defaults based on your users' geographic locations. See Geographical IDs for more information on identifying regions.});By specifying regions in the consent command, you can limit the default behavior of your tags to the specified locations, ensuring compliance with regional data protection regulations. For more nuanced control based on user location, delve into the Geographical IDs documentation to enhance your site’s regional specificity in consent management. For those employing an asynchronous CMP, the wait_for_update parameter is a valuable tool that dictates the latency before Google tags send out data, allowing the consent state to be updated. This parameter is unnecessary for synchronous platforms, where consent determination occurs instantaneously.Update consent stateAs users navigate a website, they may alter their consent preferences. The Consent Mode implementation should respond to these interactions by utilizing the update command to modify the consent status as soon as possible. Maintaining the user’s choice throughout their session is equally vital upon obtaining consent. Ensure the update command is invoked on subsequent pages to reflect their current consent status accurately.And remember, it is only necessary to issue updates when there is an actual change in the user’s consent status. Most CMPs will automatically issue an update command on every page once the user has made a consent decision.window.gtag(\"consent\", \"update\", {  analytics_storage: \"granted\",  ad_storage: \"denied\",  ad_user_data: \"denied\",  ad_personalization: \"denied\",});Set url passthroughThe url_passthrough setting in the Consent Mode framework aims to improve the accuracy of conversion tracking by enabling the passing of ad-related information through URL query parameters.gtag(\"set\", \"url_passthrough\", true);  Note: This command is usually added to the default snippet before any gtag('config',...) commands.This feature becomes relevant when a user’s consent settings prevent the storage of ad-related data (ad_storage: denied). Instead of relying on first-party cookies, which usually capture details from ad clicks, URL passthrough ensures this information is carried across pages until a conversion occurs within the URL. Hence, this method unlocks the measurement of ad click efficacy.For URL passthrough to function, several prerequisites must be satisfied:  The navigational link must lead to the same domain.  A Google Click Identifier (GCLID) or DoubleClick Click Identifier (DCLID) must be present in the URL.When the advertiser’s implementation meets these conditions, URL passthrough is a (not so robust) alternative to traditional cookies.Obtain current consent stateTo quickly ascertain the current consent state for any consent type, you can utilize the getConsentState method from the Google tag’s global object. Executing the following will return a binary value: 0 indicates no consent (denied), and 1 signifies consent has been granted:window.google_tag_data.ics.getConsentState(\"ad_storage\"); // Example for ad_storageThis provides a quick and efficient means of checking user consent in real-time using the pre-defined APIs.Listen for consent state changesTo monitor and respond to changes in user consent for ads and analytics, you can set up a listener using the addListener method of the Google tag’s object:window.google_tag_data.ics.addListener(  [\"ad_storage\", \"analytics_storage\"],  function (event) {    console.log(\"Consent state for either ads or analytics changed:\", event);  });The above code snippet adds a listener for consent states for ad_storage and analytics_storage. When a change occurs in either consent state, the callback function is executed, logging a message and the event details to the console. This enables real-time tracking and reaction capabilities of user consent preferences within your website or application.Using Google Tag Manager Sandbox APIs (web)Google also included the gtag API functionalities for Consent Mode into GTM’s Sandbox APIs, making it possible to build and use native community tags and variable templates in our GTM containers.Consider using Simo Ahava’s template for a streamlined implementation of Consent Mode v2. Available here, this template is ideally suited to orchestrate the Consent Mode implementation.Source: Own GTM SetupBelow, we have a closer look into the specific methods employed by this template or similar ones, which are examples of their utilization. For more technical details on the APIs, see here.Set default consent stateBy invoking the setDefaultConsentState method from a tag template, the default consent state is pushed to the data layer. GTM processes this update immediately after the current event, and any tags triggered by it have finished processing or when the tag processing timeout is reached, whichever comes first. The implementation ensures that the consent update is prioritized and processed in the GTM container before other items are queued in the data layer. This approach guarantees that user consent preferences are accurately reflected and adhered to across all subsequent tag executions.const setDefaultConsentState = require(\"setDefaultConsentState\");setDefaultConsentState({  ad_storage: \"denied\",  ad_user_data: \"denied\",  ad_personalization: \"denied\",  analytics_storage: \"denied\",  functionality_storage: \"granted\",  personalization_storage: \"granted\",  security_storage: \"granted\",  wait_for_update: 500, // optional  region: [\"DE\", \"US\"], // optional});This prioritization is critical to remember, as this command allows you to bypass the command queue, which, i.e., the gtag API is subject to. So, if you want to use Consent Mode with GTM, I recommend using a GTM template utilizing the respective Sandbox APIs.Including optional parameters like wait_for_update and region offers additional flexibility, allowing for fine-tuned control based on specific timing needs and geographic considerations. These parameters are optional and work as described for the gtag API.Update consent stateThe updateConsentState function allows for dynamic updating of consent states across various categories. The values for these categories can be dynamically populated, for instance, using variables that reflect the user’s choices as recorded by the CMP.const updateConsentState = require(\"updateConsentState\");updateConsentState({  ad_storage: \"granted\", // Can be dynamically populated. E.g., using a variable  ad_user_data: \"granted\",  ad_personalization: \"granted\",  analytics_storage: \"granted\",  functionality_storage: \"granted\",  personalization_storage: \"granted\",  security_storage: \"granted\",});Similar to the setDefaultConsentState, this function pushes a gtag set command to the data layer and guarantees that the updated consent state is recognized and acted upon in the GTM container before processing any other items in the data layer queue.Set url passthrough and ads_data_redactionThe previously mentioned gtag('set',...)commands, like ads_data_redaction or url_passthrough are available as well, allowing for the same functionality as the gtag API. Developers can use the gtagSet function in custom GTM templates to implement these commands.const gtagSet = require(\"gtagSet\");gtagSet(\"ads_data_redaction\", true);gtagSet(\"url_passthrough\", true);Listen for consent state changesThe addConsentListener function registers a listener that triggers whenever there’s a change in the consent status of a specified type (e.g., analytics_storage), such as shifting from “denied” to “granted” or vice versa. However, it’s important to note that this function does not activate if a previously unset consent type is updated to “granted”, as an unset state is already considered as “granted”.const isConsentGranted = require(\"isConsentGranted\");const addConsentListener = require(\"addConsentListener\");if (!isConsentGranted(\"ad_storage\")) {  let wasCalled = false;  addConsentListener(\"ad_storage\", (consentType, granted) = {    if (wasCalled) return;    wasCalled = true;    const cookies = getMyCookies();    sendFullPixel(cookies);  });}Additionally, the isConsentGranted function returns “true” if a particular consent type is deemed granted. This status is achieved either when the consent type is explicitly set to ‘granted’ or when it remains unset. Any other setting for the consent type is interpreted as not granted.Combining these two functions allows for the creation of a listener that triggers when a particular consent type is granted. This is useful for implementing a fallback mechanism for when a user grants consent for a specific type of consent.Using Firebase SDK (app)The Firebase Analytics SDK library enables you to collect behavioral data from your app for GA4. The Firebase SDK natively supports Consent Mode v2 and allows for implementing the two critical commands mentioned before. Regardless, adopting Consent Mode v2 in your app is probably the biggest challenge in the migration process, as very few companies have implemented it so far. But as of March 6th, 2024, it will be mandatory for all advertisers targeting EEA users and wanting to use all GMP functionalities - also for app tracking.Set default consent stateAs for the web, when integrating the Firebase SDK for app development, configuring the default consent state is crucial for managing user privacy preferences. Out of the box, the app does not establish any consent mode values within an application. Access your app’s info.plist file to set these defaults. Within this file, the app developer introduces consent mode key-value pairs, where each key represents a specific type of consent, and the value indicates the consent state—true for granted consent and false for denied.To exemplify, if you aim to deny consent by default for all parameters, your info.plist should include entries like the following:&lt;keyGOOGLE_ANALYTICS_DEFAULT_ALLOW_ANALYTICS_STORAGE&lt;/key &lt;denied/&lt;keyGOOGLE_ANALYTICS_DEFAULT_ALLOW_AD_STORAGE&lt;/key &lt;denied/&lt;keyGOOGLE_ANALYTICS_DEFAULT_ALLOW_AD_USER_DATA&lt;/key &lt;denied/&lt;keyGOOGLE_ANALYTICS_DEFAULT_ALLOW_AD_PERSONALIZATION_SIGNALS&lt;/key &lt;denied/Update consentAs we know, the users’ consent states must be updated after their consent decisions. This is achieved through the setConsent method within the Firebase SDK. When invoked, this method overwrites the default consent settings and ensures that the updated preferences are preserved across future app launches. These updated consent states remain active until setConsent is executed again, ensuring that changes in user preferences are respected, even if the app is closed and later reopened. setConsent allows for selective updates, affecting only the specified consent parameters - just like its gtag.js counterpart.For instance, to update all consent values to “granted” status, developers can utilize the following Swift code snippet:Analytics.setConsent([  .analyticsStorage: .granted,  .adStorage: .granted,  .adUserData: .granted,  .adPersonalization: .granted,])For more details on how to debug apps using Firebase Analytics, see here.Measurement ProtocolThe Measurement Protocol (MP) is a tool for sending event data directly to Google Analytics servers via HTTP requests, enabling enriching web and app interactions with server-side data.With Consent Mode v2, the MP incorporates a consent attribute explicitly configuring consent types and states within each request. If specific consent parameters are not included in a request, GA4 will default to the consent settings established through online interactions associated with the same client ID or app instance. Hence, setting the attribute via the MP might not be as crucial as it is for websites and apps because it can be inferred from other events.For example, when sending event data, such as a purchase event, you can specify consent states for data collection and processing through the MP. The following CURL command demonstrates how to include consent attributes for ad_user_data and ad_personalization within an event payload:curl -X POST 'https://www.google-analytics.com/debug/mp/collect?api_secret=&lt;your-key&gt;&amp;measurement_id=G-XXXXXX' \\ -H 'Content-Type: application/json' \\ -d '{   \"client_id\": \"709808103.1671720316\",   \"events\": [     {       \"name\": \"purchase\",       \"params\": {         \"items\": [           {             \"item_id\": \"SKUTest1234\",             ...           },         ],         \"currency\": \"DKK\",         ...         \"ad_user_data\": \"granted\",         \"ad_personalization\": \"denied\"       }     }   ]}'Google APIsLastly, let’s briefly look at Google’s APIs and how they are affected by Consent Mode v2. APIs are yet another way for advertisers to ingest data into Google’s system, helping them to enable remarketing use cases (e.g., Customer Match via API upload) or send offline conversions with 1PD (e.g., Enhanced Conversions for Leads). Hence, advertisers must also send users’ consent signals for these data points to ensure a comprehensively designed system from Google’s perspective.For example, the Google Ads API introduced an update by incorporating the Consent object. This object is now a prerequisite for data uploads associated with call conversions, click conversions, Customer Match, and Store Sales, in line with how consent is managed within the remaining ecosystem. Advertisers must now explicitly set consent for each data interaction, as detailed below:            Feature      Code change                  Call Conversions      Set Consent for each call conversion event at CallConversion.consent.              Click Conversions      Set Consent for each click conversion event at ClickConversion.consent.              Customer Match      Set Consent for each Customer Match job at CustomerMatchUserListMetadata.consent.              Store Sales      Set Consent for each Store Sales event at UserData.consent.      Source: Google Ads Developer BlogThis structured approach to consent within the Google Ads and other GMP APIs ensures that all data interactions (ideally) comply with user preferences. By integrating the Consent object across these features, Google provides advertisers with the tools to manage consent more effectively and forces them to think about whether or not the correct has been obtained to share this data.Validating the Consent Mode v2 ImplementationNow that we have covered the implementation of Consent Mode v2, it is time to validate the setup. The following section provides a comprehensive overview of how to validate the Consent Mode v2 implementation using various methods and tools.Using GTM Preview ModeThe GTM Preview Mode is a powerful tool for validating the Consent Mode v2 implementation. It allows you to inspect the consent state and the data layer, and to verify that the consent state is being updated correctly.GTM Web PreviewWhen using the GTM Web Preview, you can inspect the consent state in the GTM interface. The consent state is displayed in the “Consent” tab of the GTM interface, and you can see the current consent state for each consent type. Overall, the interface gives you insight into the following for every recorded dataLayer event:  On-page Default (inferred from default command)  On-page Update (inferred from update command)  Current State (consent state applicable to the current event and associated tags)Source: Own GTM SetupGTM Server-Side PreviewFor the GTM Server-Side Preview, we are missing a user-friendly interface like the one for the web preview. However, we can inspect the consent state once the GA4 client parses the incoming event data. The event data exposes the consent states as part of the x-sst-system-properties object. From here, you can use the consent state to block or allow certain tags from firing based on the user’s consent preferences. Or pass the consent state to your tags for further analysis downstream.Source: Own GTM SetupUsing the dataLayer objectSince the gtag API eventually pushes all of its events and data to the data layer, you can also inspect the consent state directly in the data layer. This is especially useful when you want to validate the consent state without using GTM (e.g., plain JS) or when you want to verify that the consent state is being updated correctly using the browser’s console.In the dataLayer, you can see the default and update commands and the respective consent states set for each type. This is a great way to verify that the consent state is being updated correctly and that the correct consent state is being applied to your tags.Source: Own GTM Setup  This will only work if you have implemented the consent state using gtag.js. If you have used the GTM Sandbox APIs, you cannot inspect the consent state as described above.Using the google_tag_data objectThe first time that I saw the google_tag_data object was when I was reading Cookiebot blog post on Consent Mode v2. This global object is available on the page and contains information about the current consent state. You can use this object to listen for changes in the consent state and obtain the current consent state for each type.Again, this is helpful if you want to use plain JS to validate the consent state and trigger or orchestrate other logic based on the user’s consent preferences.google_tag_data.ics.entriesUsing the google_tag_data.ics.entries object, you can inspect the current consent state for each consent type. This is a great way to verify that the consent state is being updated correctly and that the correct consent state is being applied to your tags. Using the following snippet, you can log the current consent state for each consent type nicely formatted to the console:(() =&gt; {  l = (s) =&gt; (s == undefined ? \"\" : s ? \"granted\" : \"denied\");  c = (s) =&gt; (s == \"granted\" ? \"color: #0C0\" : \"color: #C00\");  if (!\"google_tag_data\" in window) {    console.log(\"No Consent Mode data found\");    return;  }  var g = google_tag_data.ics.entries,    i = \"\",    t = \"\",    u = \"\";  for (var a in g) {    i = l(g[a][\"default\"]);    u = l(g[a][\"update\"]);    if (i == \"\" &amp;&amp; u == \"\") continue;    t =      \"\\t\" +      a +      \":\" +      (i != \"\" ? \"\\n\\t\\tDefault: %c\" + i : \"%c\") +      \"%c\" +      (u != \"\" ? \"\\n\\t\\tUpdate: %c\" + u : \"%c\");    console.log(t, i != \"\" ? c(i) : \"\", \"\", u != \"\" ? c(u) : \"\", \"\");  }  if (i == \"\") console.log(\"No default Consent settings found\");})();Source: CookiebotThe above snippet will print something like this to the console. Feel free to modify it as needed to suit your needs:Source: Own GTM SetupUsing the Network TabAs with every other part of the measurement setup, the Network tab is the source of truth when validating the Consent Mode v2 implementation. It allows you to inspect the requests and responses sent to and from Google’s servers - the data that is being dispatched by the browser -and verify that the consent state is being applied correctly to your tags.gcs parameterThe gcsparameter has been around since Google Consent Mode v1 and is used to communicate the current consent state for ad_storage and analytics_storage to Google’s servers. So, the consent type signals to Google whether or not cookies have been used while obtaining the event data.The gcs parameter is included in the requests sent to Google’s servers and contains the current consent state encoded in a binary format:G1&lt;ad_storage [0/1]&gt;&lt;analytics_storage [0/1]&gt;The parameter is used to signal to Google whether or not cookies have been used while obtaining the event data. See below for the possible values of the gcs parameter and their meanings:            Value      Meaning                  G100      Consent is denied for both ad_storage and analytics_storage.              G110      Consent is granted for ad_storage and denied for analytics_storage.              G101      Consent is denied for ad_storage and granted for analytics_storage.              G111      Consent for both ad_storage and analytics_storage is granted.              G1--      The site did not require consent for ad_storage or analytics_storage.        According to Google, the functionality and the format of the gcs query parameter remain as is, and Google has planned no changes to the parameter format.gcd parameterThe gcd parameter is the new kid on the block, as it has been introduced with Consent Mode v2. It is primarily used to communicate the current consent state for ad_user_data and ad_personalization to Google’s servers. But the parameter also contains the information about the consent state for ad_storage and analytics_storage:11&lt;ad_storage&gt;1&lt;analytics_storage&gt;1&lt;ad_user_data&gt;1&lt;ad_personalization&gt;5Furthermore, the gcd parameter contains information about the current state and whether or not the current state was obtained from a default or an update command or never has been actively set. To convey all of this information in a single parameter, Google uses a base64-encoded format, where each letter represents a specific consent state and command type:            Letter      Description      Consent Status      Command Type      Example                  p      All types are denied in only default setting      Denied      Default      l1p1p1p1p5              q      All types are denied in default and update settings      Denied      Default &amp; Update      l1q1q1q1q5              t      All types are granted in only default setting      Granted      Default      l1t1t1t1t5              r      All types are denied in default setting but are granted in update setting      Denied, then Granted      Default, then Update      l1r1r1r1r5              m      All types are denied in only update setting (rare)      Denied      Update      l1m1m1m1m5              n      All types are granted in only update setting (rare)      Granted      Update      l1n1n1n1n5              u      All types are granted in default setting but are denied in update setting (rare)      Granted, then Denied      Default, then Update      l1u1u1u1u5              v      All types are granted in default and update settings (rare)      Granted      Default &amp; Update      l1v1v1v1v5              l      Consent type is missing (indicates no Consent Mode setting)      Missing      None      l1l1l1l1l1      As you can see, the gcdparameter is as cryptic as it gets. But having an understanding of the parameter and its values is crucial for validating the Consent Mode v2 implementation.  Huge shoutout to Markus Baersch for deciperhing the gcd parameter and making the information available to the community. For more information, see here.Validating the Consent Mode v2 Implementation forLastly, let’s have a look at how to validate and debug the Consent Mode v2 implementation for Firebase SDK on different platforms. That much I can tell you already: it is a bit more straightforward than for web, as you can use the debug console of Xcode and Android Studio to verify the consent state.iOSYou can verify that your consent settings are working as intended by viewing the Xcode debug console for your app. To do so, you follow these steps:  Enable verbose logging on your device.  In the Xcode debug console, look for the consent type names in console logs:  ad_storage  analytics_storage  ad_user_data  ad_personalizationFor example, if ad_storage are enabled, you’ll see the following message: “ad_storage is granted.””AndroidYou can verify that your consent settings are working as intended by viewing the log messages for your app. To do so, you follow these steps:  Enable verbose logging on your device.  In the Android Studio logcat, find the log message that starts with “Setting consent”.For example, if ad_storage is enabled, you’ll see the following log message:Setting consent, ... AD_STORAGE=grantedConclusionIn conclusion, the evolution of Consent Mode into its second iteration underlines its importance in ensuring the management of consent preferences for Google’s advertising universe.It intends to allow users to navigate the digital space with increased autonomy, exercising more control over their data. Conversely, advertisers can benefit from a more refined approach to data collection that, if executed with precision and transparency. This can lead to a more trust-based relationship with their users.The two new consent types introduced with Consent Mode v2 are mainly focused on controlling data activation processes, signaling a more granular level of data management for advertisers than ever before. The previously present consent types remain and focus on how the data is collected. The necessary adoption of Consent Mode v2 will be a manageable technical challenge for advertisers, as its implementation is relatively straightforward.However, the introduction of Consent Mode v2 is not without its complexities (as indicated by the length of this blog post). As the advertising system created by Google and its legislation grows in sophistication, the burden of understanding and correctly implementing these new settings falls heavily on those who manage them - the advertisers.MarTech providers such as Google should make it as simple as possible for advertisers to follow these standards - Google needs to give Consent Mode more love and work to do that."
  },
  
  {
    "title": "GA4 Client-Side Data Redaction - How to remove PII from your data before you collect it",
    "url": "/gunnargriese.github.io/posts/ga4-data-redaction/",
    "categories": "GA4",
    "tags": "ga4",
    "date": "2023-09-26 06:21:15 +0200",
    





    
    "snippet": "In my recent article, I explored the potential of GA4 as a core component in crafting a lightweight Composable CDP. One of the capabilities for a Composable CDP is its ability to account for Data G...",
    "content": "In my recent article, I explored the potential of GA4 as a core component in crafting a lightweight Composable CDP. One of the capabilities for a Composable CDP is its ability to account for Data Governance and Privacy Controls. Building on that, this post zooms in on GA4’s capabilities that allow for data management in alignment with privacy standards. At the heart of our discussion is the Client-Side Data Redaction feature, designed to scrub out personally identifiable information (PII) before GA4’s data collection ensues.As outlined in Google Analytics’ Terms and Conditions, transmitting PII, be it names, email addresses, or even credit card numbers, is a strict no-no. Through this post, I’ll walk you through the nuances of leveraging GA4’s client-side data redaction, ensuring your data remains PII-free before reaching Google Analytics. Alongside, we’ll also uncover the mechanics behind this feature and explore more methodologies to fortify your data against PII leaks to GA4.GA4 Data Collection under the HoodInitially, let’s dissect how GA4’s data collection functions internally. The subsequent diagram illustrates the data collection pipeline of GA4 for a typical client-side measurement setup. This pipeline encompasses three phases:  Loading the tracking library (gtag.js) and the measurement logic, often through GTM.  Dispatching the intended events along with their metadata to GA4.  Processing data on GA4’s servers, where data from users’ browsers is gathered and aggregated.GA4 Data Pipeline - Own VisualizationSimilar to Universal Analytics (UA), certain configurations of the platform are responsible for how data is processed before it is exposed to the analyst. For instance, for how long data is retained (Data Rentention) settings or how to reat certain traffic types (e.g., exclude it from the data set or mark it up via a dimension). Again, this just like in UA, where Property and View settings were responsible for how data is processed before it is exposed to the analyst.A notable change in GA4 is Google’s decision to relocate specific processing logic from its servers to the client-side. My hypothesis is that this transition was made to conserve computational resources — a reasonable shift, especially considering its free availability to most users.This migration implies that the users’ browsers, as part of the data collection phase, now take over some pre-transmission processing tasks. This encompasses areas like sessionization logic. Contrary to UA where sessionization was managed server-side, GA4 delegates this responsibility to the browser. Here, the browser discerns session commencements and terminations, storing session identifiers in cookies and transmitting them with events to GA4. The GA4 endpoint then uses these identifiers to classify events into new or existing sessions, bypassing the need for rolling windows in the processing pipeline. Also, other configurations like IP-based internal traffic detection, new feature implementations like event modification, and event marking as conversions now occur within browsers.The analytics team, using the GA4 Admin panel, sets these configurations by defining the property and/or data stream settings. These configurations are then dispatched to users’ browsers through the gtag.js library, either embedded directly or via the GA4 Configuration tag/Google tag through GTM. For UA, a universal tracking library (analytics.js) was in use. The JavaScript (JS) within was uniform for every UA user. In contrast, GA4’s request to fetch the gtag.js library now carries the Data Stream’s Measurement ID (/gtag/js?id=G-XXXXXX). This ensures that the gtag.js library aligns specifically with your GA4 property and data stream, a logical transition considering the aforementioned.This paradigm shift, while presenting its own set of challenges, also allows us to get deeper insights into GA4’s internal mechanics. For example, we can now dissect the gtag.js library to understand the functionality of the newly introduced Client-Side Data Redaction feature. This exploration will be the focus of our subsequent section.Client-Side Data Redaction in GA4In Google Analytics 4 (GA4), we now have the flexibility to configure Data Redaction specifically for Web Data Streams, focusing on predefined text patterns like email addresses and URL query parameters. This feature “leaked” two months ago already, when Google’s official GA4 demo property was granted access to the feature. However, it was only recently that the feature has become available to more and more GA4 users (it appears to be part of a gradual roll-out effort).When it comes to email redaction, this feature operates across the entire GA4 event on a best-effort basis, ensuring a layer of privacy and security. On the other hand, URL Query Parameters specifically target a select set of event parameters for which GA4 will attempt to redact values of the specified query parameters. The URL query parameter redaction is currently supported for the following event parameters (all URL-related parameters):  page_location  page_referrer  page_path  link_url  video_url  form_destinationHow to enable Data Redaction in GA4To enable Data Redaction, navigate to the Admin panel of your GA4 property and select the desired Web Data Stream. Here, you’ll find the Data Redaction tab, where you can enable the feature and specify the parameters you’d like to redact. You can choose to redact email addresses and/or URL query parameters. For the latter, you can specify the parameters you’d like to redact. For instance, if you’d like to redact the firstname and lastname parameters, you can enter them in the text box, separated by commas.Activating Client-Side Data Redaction in GA4 is straightforward. Simply follow these steps:  Access the Admin panel of your GA4 property.  Choose the Web Data Stream you wish to modify.  Locate the Data Redaction tab.  Here, you can activate the Data Redaction feature and delineate the specific parameters to redact.GA4 Data Redaction InterfaceWithin the settings you’ll also have the ability to preview the effect of the Data Redaction settings:GA4 Data Redaction PreviewIt’s important to note, however, that this feature is exclusive to Web Data Streams and is not available for App Data Streams. Additionally, if you are using pre-existing Web Data Streams, both email and query parameter redaction features will be turned OFF by default. For newly created Web Data Streams, Email redaction will be enabled automatically, while Query Parameter redaction will remain OFF. This design provides a customizable approach to data handling, ensuring GA4 users can adeptly navigate between utility and privacy.Effect of Data Redaction on Data CollectionLet’s have a look at how the Data Redaction feature affects data collection. To do so, we’ll use the following example, where I have configured the Data Redaction feature to redact email addresses. I’ve also configured a GA4 Configuration tag in GTM to send a page_view event to GA4. The tag is configured to send the following values for some custom event parameters:GTM Test ConfigurationAs you can see, the values represent various combinations of email addresses and strings. The following table shows the values that are sent to GA4, along with the redacted values:            Original Value      Redacted Value                  testmail@gunnar.com      (redacted)              separate_str testmail@gunnar.com      separate_str (redacted)              joined_strtestmail@gunnar.com      (redacted)              testmail@gunnar.comjoined_strtest      (redacted)_strtest      The data redaction logic replaces email addresses with the string (redacted). It’s important to note that the redaction logic for email addresses is applied to the entire event and all of its parameters. For instance, an email address detected in the search_term parameter will be redacted, even if the search_term parameter is not specified in the Data Redaction settings. Search terms containing PII like email addresses is something I’ve seen very often, having a simple to configure “global” setting preventing is a nice addition. This scope does not apply for the URL query parameter redaction feature though. If you specify a query parameter to be redacted, the redaction logic will be applied to only the previously mentioned URL-related event parameters, even though they are not explicitly specified in the Data Redaction settings.Components of the Data Redaction FeatureLet’s have a closer look at what Google means by “best-effort basis”. As mentioned earlier, we can expect this logic to present in gtag.js and when searching my specific instance of the code, we indeed find the following part of the code that holds the specifications from my web data stream:{  \"function\": \"__ccd_auto_redact\",  \"priority\": 1,  \"vtp_redactEmail\": true,  \"vtp_redactQueryParams\": \"zipcode,creditcarddetails,password,firstname,lastname\",  \"vtp_instanceDestinationId\": \"G-XXXXXXXXXX\",  \"tag_id\": 12}Furthermore, we find two regular expressions (regex) in the gtag.js-file that appear to be linked to the data redaction feature:  [A-Z0-9._%+-]+@[A-Z0-9.-]+\\\\.[A-Z]{2,} (case insensitive)  [A-Z0-9._-]|%25|%2B)+%40[A-Z0-9.-] (case insensitive)Let’s break down the regex to understand what it does:  [A-Z0-9._%+-]+: This matches one or more occurrences (+) of uppercase letters (A-Z), digits (0-9), dots (.), underscores (_), percent signs (%), plus (+), or hyphens (-). It corresponds to the “user” part of an email address (e.g., the john.doe in john.doe@example.com).  @: This matches the “@” symbol, which is a required part of an email address.  [A-Z0-9.-]+: This matches one or more occurrences (+) of uppercase letters (A-Z), digits (0-9), dots (.), or hyphens (-). It corresponds to the “domain” part of the email address (e.g., the example in john.doe@example.com).  \\\\.: This matches a single dot (.). The backslashes are used to escape the dot, as it is a special character in regex.  [A-Z]{2,}: This matches two or more occurrences ({2,}) of uppercase letters (A-Z). It corresponds to the “top-level domain” (e.g., com, org, net) part of an email address (e.g., the com in john.doe@example.com).The purpose of this regular expression is to match email addresses that follow a specific pattern. It only matches email addresses that consist of letters, numbers, and certain special characters. It expects the top-level domain to have at least two characters. It also expects the email address to have a “user” part, an “@” symbol, a “domain” part, and a “top-level domain” part.GA4’s data redaction treats percent-encoded URL query parameters as well, including Unicode characters accepted by browsers. This is what the second regex is for. It’s a variation of the first regex, but it matches percent-encoded characters. The percent-encoded characters are represented by the %25 and %2B parts of the regex.CaveatsIn terms of operation, Data Redaction kicks in after Event Modification &amp; Creation. This means that any rules you’ve set to modify or create events will be executed on the website before the Data Redaction process starts. It’s also important to be cautious, as there is a risk of unintended data redaction. For instance, certain parts of the URL or user configurations provided in tags could be mistakenly redacted if they resemble email addresses:  Data redaction may incorrectly interpret text as an email address and redact the text; for example, if the text includes “@” followed by a top-level domain name (e.g., example.com) it may be incorrectly removed.Historically, similar data manipulation could be achieved in your GTM web container by tweaking the page_location or leveraging GTM Server-Side (sGTM) transformations. Still, remember that sensitive parameters dispatched via HTTP headers, like IP addresses, referers, and user-agents, demand special attention and handling within sGTM.While the Data Redaction feature in GA4 offers you greater control over the data collected, it’s crucial to understand that this is not a one-size-fits-all solution for ensuring compliance with all laws and regulations. For a comprehensive understanding of best practices and controls for safeguarding data, I strongly recommend working together with your legal department.Lastly, it’s worth noting that this feature will not prevent data collection via other methods like Measurement Protocol (MP) or Data Import. Therefore, while Data Redaction is a useful tool, it should be part of a broader data management and compliance strategy.Other Privacy Controls in GA4Hence, let’s have a look at additional approaches that you can use to ensure that you do not send any PII to GA4.Regional Data Controls in GA4While Client-Side Data Redaction is great to control what data is sent with the GA4 event, other metadata of the request to GA4, e.g. IP addresses and user-agents, might require further treatment as well. Furthermore, the usage of Google Signals might be something that you want use only under specific circumstances.With GA4, you’re empowered with Regional Data Controls that allow you to define the specificity of location and device data you collect from your website users. Opting to disable this collection ensures that city-level location insights (derived from IP addresses) and certain device metadata (dervied from user-agent headers) are redacted even before they reach GA4’s servers. For a deep-dive into these controls, visit the official documentation on Regional Controls &amp; Google Signals.Moreover, GA4 offers advanced configurations, both in gtag.js &amp; GTM web containers, to fine-tune ad personalization and the usage of Google Signals: allow_google_signals &amp; allow_ad_personalization_signals.Within the Property Settings, you can harness these advanced configurations to either enable or disable ad personalization for users from specific countries or US states. It’s important to note that these settings are forward-looking, meaning they won’t alter previously collected data. Opting to turn off ad personalization doesn’t inhibit your capability to use the analytics data from that property for other purposes, like A/B testing in tools like Optimizely, AB Tasty, etc. or Firebase.Should you decide to disable ad personalization for any region, all events harvested from that area will be flagged as ineligible for ad personalization (NPA). This ensures that conversions from these regions remain untouched for ad personalization, even when transferred to linked ad accounts. Furthermore, users from a deactivated region won’t be added to any lists designated for export to your linked ad accounts. However, previously exported lists remain unchanged.Transformations in GTM Server-SideTransformations in a GTM server-side container serve as a powerful tool, granting you the capability to shape the event parameters emitted from your GA4 client before they’re channeled to GA4 (and other) tags. This not only lets you protect sensitive data but also allows you to meticulously dictate which event parameters and HTTP headers are up for subsequent processing.Source: Official GTM Server-Side DocumentationWith transformations, you can:  Define Precision: Share only those event parameters with tags that you’ve explicitly marked.  Augment Data: Craft rules to modify existing event parameters or introduce new ones.  Maintain Privacy: Proactively redact information by keeping certain event parameters away from tags.The flexibility of transformations is tremendous. You can implement them across all your tags, specific tag types, or even a custom-selected group. Moreover, you can establish a series of conditions, ensuring transformations are executed only when these criteria are satisfied.For those keen on upholding rigorous privacy standards in data collection, routing GA4 events through sGTM is a wise strategy. Furthermore, when dealing with server-to-server events, routing MP events through sGTM can ensure that the same redaction logic applies as for events collected client-side. Therefore, I strongly recommend harnessing transformations to redact PII to ensure that no personal data inadvertently finds its way to GA4.Data Deletion Requests in GA4In case sensitive data slipped through your previous safeguarding measurements, GA4 offers a convenient solution: the data-deletion request. By leveraging this feature, you can surgically remove text data accumulated through event parameters from GA4 servers after they’ve been collected. Notably, even though the original text will be replaced with “(data deleted)”, the event’s impact remains intact in your overall report metrics.GA4 provides you with four options to initiate data deletion requests:            Deletion Type      Effect on Data                  Delete all parameters on all events      This option deletes all registered and automatically collected parameters across all collected events.              Delete all registered parameters on selected events      This option deletes all registered parameters collected across a list of events you select in the next step.              Delete selected parameters on all events      This option deletes registered parameters that you select in the next step across all collected events.              Delete selected parameters on selected events      This option deletes registered parameters that you select in the next step across a list of events that you also select in the next step.              Delete selected user properties      This option deletes user properties that you select in the next step      Source: Data-deletion requests documentationGA4 accommodates the removal of both automatically recorded parameters and those manually registered as custom dimensions. However, it refrains from erasing numeric parameters, text parameters based on trusted internal identifiers, and the preset entries “”, “(not set)”, and “(data deleted)”. Auto-collected Parameters can be removed exclusively when you opt to erase all parameters across every event. Analytics can wipe out custom parameters tagged as custom dimensions. If you can’t spot a specific parameter in the deletion list, it’s likely it was never measured initially. Double-check the parameter’s name and its registration status before deletion.It’s worth noting: if you’ve established a custom dimension that sources its data from an auto-collected parameter, removing the custom parameter won’t affect the auto-collected one.GA4 User Deletion APIA somewhat related data control in GA4, but one of the lesser known ones is the User Deletion API. Its impact on your data is more rigorous than for Data Deletion requests, as this API empowers you to facilitate the removal of all data linked to specific user identifiers. So, if any of the aforementioned methods failed and unwanted data slipped through, the User Deletion API serves you as a tool to delete data related to designated user identifiers found in your properties. Initiating these deletion requests is feasible through both Firebase projects and GA properties. To initiate a User Deletion API request, you must specify the user identifier type and the user identifier itself.Supported user identifier types include:  CLIENT_ID: Pertains to the Google Analytics Client ID.  USER_ID: This is the Google Analytics User ID.  APP_INSTANCE_ID: Represents the Firebase application instance ID.It’s crucial to understand that each data deletion request exclusively addresses the identifier mentioned in that particular appeal. For users linked with multiple identifiers, distinct deletion requests for every associated identifier are mandatory. If your analytics also integrates with BigQuery exports, ensure you handle deletions on that platform too.Post the initiation of a user deletion request, the corresponding user identifier data will vanish from the User Explorer Report within a span of 72 hours. Subsequently, the data will be purged from Analytics servers during the following deletion cycle, which typically takes place roughly bi-monthly. If you’ve moved any of this data from Google Analytics to platforms like BigQuery, you might want to consider deleting the related identifiers there as well by running data manipulation statements.It’s worth noting that reports generated from previously consolidated data, like user counts in the audience overview report, will remain intact and unaltered - only the granular data will be removed.Overview of Privacy Controls in GA4When we look at the privacy controls in GA4, we can see that they are spread across different areas of the platform and the data flow. The following diagram provides an overview of the different privacy controls in GA4 and where in the data flow they are applied:Overview of GA4 Privacy Controls - Own Visualization  Pre-Data Collection: This is where you prevent data from being picked up client-side altogether. Here you can utilize the Data Redaction feature or apply your own redaction logic using custom variables in the GTM web container.  In Transit: This is where you scrub data from the GA4 requests before they hit GA4’s servers. Here you can configure Regional Data Controls and take advantage of sGTM’s Transformations feature.  Post-Data Collection: This is where you delete already collected data. Here you can utilize Data Deletion Requests and/or the User Deletion API.For a complete rundown of all available data controls in GA4, I recommend consulting this official Google document, since it offers a central guide to GA4’s data practices and controls for protecting the confidentiality and security of data collected by GA.ConclusionThe ever-evolving landscape of data privacy and governance has prompted platforms like Google Analytics 4 (GA4) to introduce robust measures ensuring data sanctity. GA4’s suite of privacy controls, ranging from Client-Side Data Redaction to the User Deletion API, empowers GA4 users with tools that allow for fine-grained data protection. With features like Client-Side Data Redaction and transformations in GTM Server-Side, organizations can meticulously regulate the flow and processing of data, striking a balance between actionable insights and data privacy. While I like to see Google’s effort in making compliance easier to manage and thereby more accessible to the broader public, I’d love to see more flexibility in the future. For instance, I miss the ability to define my own regular expressions to match my specific use cases - similar to the customTask feature we used to have for Universal Analytics. Unfortunately, limited customization possibilities appear to be a trend in GA4, as we’re seeing it in other areas as well.However, while these tools are incredibly powerful, they are not silver bullets. It’s imperative for businesses and analysts alike to understand their intricacies and applications, ensuring they align with legal regulations and organizational standards. The obligation falls on organizations to stay updated, as platforms, regulations, and standards continuously evolve. It’s equally vital to collaborate closely with legal and compliance departments, ensuring that the usage of these tools aligns with overarching data governance policies.GA4’s advancements, covered extensively in this blog post, are steps forward. They reflect a broader industry shift towards transparency, accountability, and user-centricity. By leveraging these controls effectively and responsibly, organizations can not only uphold their reputation but also foster trust among their user base, a cornerstone for long-term success in today’s digital age."
  },
  
  {
    "title": "GA4 - The CDP You Didn't Know You Had",
    "url": "/gunnargriese.github.io/posts/ga4-the-cdp-you-didnt-know-you-had/",
    "categories": "GA4",
    "tags": "ga4, bigquery, gtm-server-side",
    "date": "2023-08-12 12:14:15 +0200",
    





    
    "snippet": "This article is a write-up of my talk at MeasureCamp Czechia in Prague in September 2023 - thanks to everyone who attended the session and provided feedback.I’ll explore how Google Analytics 4 (GA4...",
    "content": "This article is a write-up of my talk at MeasureCamp Czechia in Prague in September 2023 - thanks to everyone who attended the session and provided feedback.I’ll explore how Google Analytics 4 (GA4) can be used as a lightweight Customer Data Platform (CDP) and how it can be integrated with other tools to create a powerful data activation stack. Furthermore, I’ll also discuss the current state of GA4 and the challenges faced by GA4 users. Lastly, I’ll highlight some of the recently added powerful GA4 features like the Audience Export API and User data exports to BigQuery and how they can be used to build a use case-specific data activation stack (or “Customer Data Platform (CDP)” if we want to join the buzzword bingo).Walk Down Memory Lane - History of Google AnalyticsFirst let’s take a step back to put the recent developments into perspective and look at some high-level context. Google Analytics (GA) has come a long way since its inception in 2005. Back then, GA was a simple JavaScript snippet that allowed website owners to track pageviews and gain insights into their website’s performance. Over the years, GA has evolved into a sophisticated analytics platform that offers a wide range of features and capabilities.In March 2016, Google introduced Google Analytics 360, a premium version of GA that offers additional features and support. In 2017, Google launched Google Analytics for Firebase, a free analytics solution for mobile apps. And in 2018, Google re-strucutred its marketing platform, which resulted in GA becoming a part of the Google Marketing Platform (GMP). In the GMP, GA takes the role of a central measurement platform able to share audiences and conversions with the marketing tools in the platform to help optimize ad budgets. In 2019, Google announced Google Analytics 4, a new version of GA that offers a more unified approach to analytics across platforms and devices, which replaced its predecessor Universal Analytics (UA).Google built GA4 on the foundation of Google Analytics for Firebase, and since allows users to combine data from websites and mobile apps into a single property. Moreover, GA4 made former 360-features part of its free version, such as BigQuery export, data-driven attribution, and more. The arrival of GA4 also marked a shift in how Google thinks marketers should approach analytics and data activation, as it sets a stronger focus on the Google Cloud Platform (GCP) and its capabilities. BigQuery is considered the centerpiece of this new approach, as it allows users to export raw data from GA4 and use it for advanced analyses - oftentimes being the only way to overcome limitations imposed in GA4’s interface (e.g., limits due to cardinality or privacy restrictions). At the same time, other solutions which are tightly integrated with GA4 and harness the capabilities of the GCP evolved, like Google Tag Manager Server-Side, which allows users to send data from their website to GA4 via the GCP (or any other cloud provider).The Current State of GA4 and Challenges Faced by GA4 UsersWhile Google touted GA4 as a revolutionary step forward in analytics, it’s important to acknowledge the platform’s various shortcomings that have made it difficult for the community to fully trust and utilize the data it provides. Many users (including me) have felt like guinea pigs, navigating and debugging a flawed tool on behalf of Google’s product team with minimal support and missing documentation. This has led to a significant amount of lost trust and time being spent on troubleshooting — trying to discern whether issues arose from setup errors or inherent bugs within the tool itself. The situation is exacerbated when you compare the learning curves of GA4 and UA. While UA had a steady, albeit less steep, learning curve, GA4’s has been steeper and less well-supported, both by Google and the broader analytics community since it simply was a new tool nobody had any experience with.The plentitude of blog posts addressing specific issues with GA4 and its usability is a testament to the tool’s current shortcomings. See below for a few selected examples:  The Traffic Source Challenge in GA4 (by Charles Farina) explains how a supposedly simple question like “How many users visited your site?” can be difficult to answer in GA4.  GA4 sessions magick. Which hit makes a session source / medium? (by Artem Korneev) explores how GA4’s sessionization logic, which is not inherent to GA4’s data model, works and how it can lead to unexpected results.  How Do I Access The Individual Timestamp Of A GA4 Event? (by Simo Ahava) explains how GA4’s tracking library attempts to batch events together and how this makes it impossible (without a workaround) to access the individual timestamp of a GA4 event.  Lastly, Dear Google Analytics 4 (by Simo Ahava) is a letter to Google Analytics 4, where Simo Ahava shares his thoughts on the current state of GA4 and the challenges it poses to users.All of this, has made it challenging for users to focus on data analysis and activation, rather than getting bogged down by the tool’s limitations. However, it’s important to note that GA4 is still in its infancy (which it obviously shouldn’t be anymore) and that Google is working hard to improve the tool - which is proven by the frequent feature releases and improvements.That said, no platform is without its challenges, whether it’s data migration issues or problems with integrating other tools. Despite these hurdles, the focus of this blog post is to explore recently added GA4 capabilities and the “cool” stuff you can do with the tool. I truly believe it’s about time to stop working the tool and rather start exploring what the tool can do for us!So, while it’s crucial to be aware of these issues, let’s dive into the capabilities that GA4 and its deeper integration into the GCP bring to the table.Recently Added Powerful GA4 Features for Data ActivationUser Data Export to BigQueryIn your journey with GA4, you have probably already come across the BigQuery Export feature that allows you to export (almost) raw-level event data (if not, I strongly recommend checking it out). In August 2023, Google added the option to include a daily export of user data, which is then organized into two distinct tables within your BigQuery project under the same dataset as the events_* and events_intraday_* tables.Source: Google DocumentationThe first table, labeled pseudonymous_users_* contains a row for each pseudonymous identifier and is updated whenever there’s a change in one of the fields. Importantly, this table does not include data for unconsented users or export user IDs, but it does include the last active timestamp for each pseudo ID. The second table, users_* focuses on user-specific data, updating similarly when there’s a field change. Unlike the Pseudo ID table, this one can include data for unconsented users if a user ID is present (which might be considered somewhat problematic from a privacy perspective).Both tables include any user whose data has changed that day. This could be due to a new session initiation or even being dropped from an audience for not meeting certain conditions, like not making a purchase in the last 7 days. For more information on how to work with these datasets, check out Google’s query cookbook.Source: Google DocumentationWhile the exports contain a lot of expected data points like user properties, device information, and geo data, I am mostly hyped over the Audiences, Lifetime and Predictions fields.The Audiences fields allow you to query a list of User or User Pseudo IDs that are part of a specific audience. This is a great way to save time when attempting to recreate complex audience definitions from the GA4 Audience Builder in BigQuery. Previously you had to export the audience definitions from the GA4 UI and then recreate the actual audience in BigQuery. Now you can simply query the audiences.id or audiences.name fields and get a list of User or User Pseudo IDs that are part of a specific audience. Furthermore, you might want to share these IDs with other tools in your data activation stack, like a CRM or a DMP.The Lifetime fields contain user-level totals for revenue, engagement time, numer of purchases, etc.. This is super helpful if you plan to apply behavioral segmentation techniques to your data. Essentially, you can use these fields to create a user-level RFM (Recency, Frequency, Monetary) model, which can be used to identify your most valuable customers and create personalized experiences for them.Lastly, the Predictions fields contain user-level predictions for the likelihood of a user to perform a specific event. This is a great way to identify users that are likely to perform a specific action, like making a purchase, and then target them with a specific message or offer. Again, having this data at your fingertips out of the box saves you the time and effort of recreating these predictions in BigQuery using BigQuery ML or building out your own pipelines using something like Vertex AI.  Please be aware that at this point of time the exports seem to include data from just a subset of the site’s actual visitors though (a typical Google release?). So, this feature does not seem to be fully functional yet.But eventually these exports will hopefully reduce the effort to obtain audience data significantly and enable the use cases mentioned above.Audience Export APIWith GA4, Google introduced a new Audience Builder that allows you to create complex audiences based on a wide range of conditions and much more flexible than what you could do in UA. While this was a great step forward, it still was a rather cumbersome process to export these audiences to other tools in your data activation stack, which is why only very few companies did it in the first place. Out of the box, you could export audiences to Google Ads and other GMP tools, but that was about it.Source: LinkedInIn June 2023, the Audience Export API was released and coined as “one of the most transformative API releases in the history of GA”. The reason for this is that the introduction of this API marks a strategic shift for GA from being a data silo to a data activation platform. The API allows you to export audiences to other tools in your data activation stack, and use them to create personalized experiences for your users. Early adopters of this functionality are third-party A/B testing tools like Optimizely, VWO, or AB Tasty. These tools allow you to create personalized experiences for your users based on the audiences you created in GA4. For example, you could create an audience of users that have not made a purchase in the last 7 days and then target them with a specific offer or message.Working with GA4 Audience ListsThe following process illustrates how you can use the Audience Export API to export audiences to any tool or platform of your liking:Source: Own visualization and Audience List Fundamentals documentationAudiences allow you to group users based on shared attributes, providing a more targeted approach to data analysis and marketing. The API enables you to create Audience Lists asynchronously, offering snapshots of users within a specific audience. You can initiate this by making a POST request to the audienceLists.create method, followed by querying the list using audienceLists.query. Metadata and configurations can be retrieved using audienceLists.get, and you can view all Audience Lists for a property with audienceLists.list.The API requires the GA4 property identifier in the URL request path. The Audience List creation process requires specific parameters, such as a valid audience name and one dimension (userId or deviceId). Once created, it may take several minutes for the Audience List to be generated, and you can check its readiness state using audienceLists.get.GA4 Audience Lists LimitationsAudience Lists identify users by User ID or Device ID, regardless of the reporting identity setting in GA4. This means that the number of users in your reports can differ from those in your Audience Lists. Also, unlike Google Ads remarketing, Audience Lists do not contain backfilled audience memberships. Users need to log an event after the audience is created to appear in new Audience Lists.  It’s important to note that Audience Lists are snapshots in time and do not automatically update. They have a data freshness period, typically averaging 24 hours, and expire after 72 hours. If you need updated lists, you’ll have to create new ones. Additionally, there are user limits depending on your property type, affecting the maximum number of returned and considered users.So, while the process has its intricacies, the ability to create and query Audience Lists in GA4 offers a powerful tool for businesses looking to understand and engage their user base more effectively.GTM Server-Side Firestore IntegrationThe Google team has recently rolled out asynchronous variables in Google Tag Manager Server-Side (sGTM), opening up new avenues for data enrichment. Adding to this the team introduced a new Google Cloud Platform API for sGTM — Firestore. This NoSQL, scalable database is designed for near-real-time write/read operations. To simplify data retrieval, sGTM now includes a Firestore Lookup variable, allowing you to pull values from specific keys or fields in a Firestore document to enrich data streams routed through sGTM.Source: Own visualizationFirestore’s potential for data enrichment is vast. You could for example build a flow that powers Value-Based Bidding in Realtime to enrich Google Ads data in real-time. “Simply” use the Firestore Lookup variable to retrieve products’ profit margin from a Firestore document and then send it to Google Ads via a server-side conversion event. This allows you to adjust your bids in real-time based on the purchased products’ profit, which is a great way to optimize your ad spend.In the context of this blog post, we might want to use the Firestore Lookup variable to enrich GA4 data with the outcome of cloud-based data modelling. For example, we could use the Firestore Lookup variable to retrieve a user’s lifetime value (LTV) or its RFM segment from a Firestore document and then send it to GA4 via a server-side event. This allows us to use this data as an input for even more valuable segment/audience definitions, which we then can activate through the Audience Export API or the User Data Exports.GA4 as a Lightweight Composable CDPNow that we’ve explored some of the recently added powerful GA4 features, let’s take a step back and look at the bigger picture. As mentioned in the introduction, GA4 allows you to combine data from websites and mobile apps into a single property, which is a great step forward. However, GA4 is still a measurement tool at its core. That said, GA4 can become the centerpiece of a data activation stack.Putting Together the Building BlocksTo achieve this, we need to combine GA4 with GCP capabilities that allow us to orchestrate data flows and build out our own data pipelines. The following diagram illustrates how we can piece together the previously mentioned features to create an engine that’s able to power a wide range of data activation use cases:Source: Own visualizationThe core elements of the proposed architecture can be summarized as follows:            Component      Purpose      Google Feature                  Behavioral Data Collection      Collect behavioral data or event data from first-party data sources      gtag.js, Google Tag Manager (GTM), Firebase SDK              Data Ingestion: ELT (or ETL)      Extract all types of data from a growing catalog of secondary data sources      GA4 User Data &amp; Raw Data Export, and BQ Data Transfer Service (Google Ads, DV360, etc.) + others (e.g. Fivetran)              Data Storage/Warehousing      Centerpiece to which all other components connect to      BigQuery              Identity Resolution and Profile API      Unifying user records captured across multiple sources      GA4 User ID (deterministic) + others (based on needs)              Visual Audience Builder (and Data Modeling)      Drag-and-drop interface to build audiences or segments by combining data from various sources      GA4 Audience Builder              Reverse ETL      Moving data from the data warehouse to downstream destinations      Audience Export API + others (e.g., Hightouch)              Data Quality      Ensure that the data powering their CDPs is not funky      DataPlex, Dataform + others (based on needs)              Data Governance and Privacy Compliance      Set up governance checks and compliance workflows      GA4 Data Controls, Consent Mode + others (based on needs)      Source: Composable CDP vs. Packaged CDP + Own interpretation of available GA4 &amp; GCP featuresThe architecture components outlined here closely resemble the modular nature of a Composable CDP. In a Composable CDP, each component is designed to perform a specific function and can be easily interchanged or augmented. For example, the “Data Ingestion: ELT (or ETL)” and “Visual Audience Builder (and Data Modeling)” components in this architecture can be tailored to integrate a variety of data sources and modeling tools, offering the adaptability that is a hallmark of Composable CDPs. The “Reverse ETL” component ensures that data can flow not just into, but also out of the data warehouse, enabling a dynamic, two-way data exchange with other platforms. This modular approach adapted by GA4 allows for greater flexibility and scalability, making it easier to adapt to evolving business needs and technologies and in a lot of cases might be able to cover the role of a dedicated Reverse ETL tool.Orchestrating Data Flows with GA4 and GCPAs you can see from the above, GA4 can be used to orchestrate data flows between various tools in your data activation stack. For example, you obviously could use enriched GA4 data to export audiences to Google Ads and DV360, and then use these audiences to create personalized experiences for your users. At the same time, you could use GA4 to export audiences to BigQuery via the User Data Export or via a direct integration with the Audience Export API, and then use these audiences to create personalized experiences for your users in your CRM or Email Marketing platform. Moreover, you could use GA4 to export audiences to BigQuery, enrich the behavioral data with first-party data from the CRM, Payment System, etc., share it with the desired ad platforms, and then use these valuable audiences to tailor your messaging towards them. As you can see, the possibilities are endless, and it’s up to you to decide which use cases are most relevant to your business.GA4 opening up its boundaries and allowing you to use its powerful features (e.g., Predictive Metrics) to inform third-party systems really is a game-changer and should change the way we think about GA4, as it is now able build out CDP use cases with minimal overhead and without the need to buy into additional tooling. This combined with lowered entry barriers to machine learning techniques (using e.g. BQML for clustering and/or prediction) makes advanced use cases with the potential to yield actual business outcomes much more accessible to a broader range of companies.ConclusionIn conclusion, Google Analytics 4 (GA4) is evolving into a robust platform capable of serving as the centerpiece of a data activation stack. However, it’s important to note some limitations. One significant shortcoming is the lack of real-time support; both the Audience Export API and User Data Export features are updated only on a daily basis, which may not meet the needs of businesses requiring real-time data activation. Additionally, the complexity of integrating these new features means that businesses will need a capable data team to stitch all the requisite components together, which can be a significant undertaking.As for areas of improvement, GA4 could benefit from more powerful Data Import features, perhaps through an API. The current method of importing data via an SFTP server is cumbersome and less than ideal. Furthermore, it would be advantageous if BigQuery could serve not just as an export destination but also as an import source for GA4, streamlining the data flow between platforms.Despite these challenges and areas for improvement, GA4’s recent feature additions and deeper integration with the Google Cloud Platform signify a strategic shift towards becoming a more open and versatile platform. These features empower businesses to enrich their data and integrate GA4 with a variety of tools, enabling a wide range of data activation use cases that go beyond traditional analytics. As GA4 continues to mature, its expanding role in data activation is defnitely something to keep an eye on if your using GA4. Feel free to reach out to me with comments or your thoughts on the above!"
  },
  
  {
    "title": "GA4 Time Travel - Bridging UTC and Local Timezones",
    "url": "/gunnargriese.github.io/posts/ga4-time-travel-bridging-utc-and-local-timezones/",
    "categories": "GA4",
    "tags": "ga4, bigquery",
    "date": "2023-08-09 09:14:15 +0200",
    





    
    "snippet": "Navigating the intricacies of data in GA4’s raw-data has its own hurdles. A good example is the event_timestamp which is logged in microseconds and set in Coordinated Universal Time (UTC). While th...",
    "content": "Navigating the intricacies of data in GA4’s raw-data has its own hurdles. A good example is the event_timestamp which is logged in microseconds and set in Coordinated Universal Time (UTC). While this standardized approach ensures consistency, it does not always align with the real-world context of events, especially when trying to make the BQ data match the data obtained from the GA4 UI. Hence, converting this timestamp to a property’s respective timezone becomes an important step for analysts. Without this conversion, analysts might misinterpret the timing of user interactions, leading to potential inaccuracies in data-driven decision-making.Take for example below graph, where we compare the number of pageviews per hour between the GA4 UI and BQ. The data from the GA4 UI is based on the property’s timezone, while the data from BQ is based on UTC. As a result, the data from BQ is shifted by 2 hours, leading to a discrepancy between the two datasets. This is because the property’s timezone is set to UTC+2 (Denmark/Copenhagen), while the data from BQ per default is set to UTC.Source: Own datasetUnderstanding GA4’s Microsecond Timestamp in BigQueryWithin the raw data export to BigQuery, GA4´s event_timestamp is expressed in microseconds. To put it into context, 1 second is divided into 1,000,000 microseconds. Originating from the Unix epoch — a fixed point in time starting from January 1, 1970 — this timestamp captures the moment an event is logged on GA4’s servers. Having this level of precision makes sense in the context of digital analytics, where events often occur in rapid succession.In essence, every event is timestamped with microscopic precision (with a slight delay given the amount of time it takes the event to reach the GA4 server), offering analysts a detailed temporal footprint of all user activities. Additionally, UTC serves as the world’s time standard, ensuring a consistent reference point across global systems. By combining the precision of microseconds with the universality of UTC, GA4 can capture and standardize events with great accuracy.  Be aware that the event_timestamp in BigQuery does not equal the time the event occured on the user’s device. It is the time the incoming event is processed by GA4 servers.  Additionally, keep in mind that GA4’s tracking library attempts to batch events together. The events in a given batch share the same event_timestamp and as of now there is no way to tell in which order these originally occured. Read more about this phenomenon and how to tackle it here.GA4’s BigQuery export schema provides the following time-related fields:            Field name      Data type      Description                  event_date      STRING      The date when the event was logged (YYYYMMDD format in the registered timezone of your app).              event_timestamp      INTEGER      The time (in microseconds, UTC) when the event was logged on the client.      Source: https://support.google.com/analytics/answer/7029846?hl=enWhile the event_date is expressed in the property’s timezone, the event_timestamp is expressed in UTC. This is important to keep in mind when working with the data and can result in unexpected discrepancies when comparing them using them in their raw form:SELECT  PARSE_DATE('%Y%m%d',event_date) AS table_date,  EXTRACT( date  FROM    TIMESTAMP_MICROS(event_timestamp) ) AS timestamp_date,  COUNT(*) AS pageviewsFROM  `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`WHERE  _TABLE_SUFFIX BETWEEN '20230701'  AND '20230703'  AND event_name = 'page_view'GROUP BY  1,  2ORDER BY  1,  2 ASCThe query above obtains the event_date and event_timestamp from the GA4 BQ export schema, converting the event_timestamp to a date format. We achieve this by first applying BQ’s TIMESTAMP_MICROS function to the event_timestamp column. The resulting TIMESTAMP value is then passed into BQ Standard SQL’s EXTRACT function, which allows us to extract various parts of the input timestamp (e.g., HOUR, DAY, DAYOFWEEK, MONTH, YEAR, etc.).In this case, we extract the date from the timestamp using EXTRACT(date FROM TIMESTAMP_MICROS(event_timestamp)). This results in a date format that matches the event_date format (YYYYMMDD), allowing us to compare the two fields.The conversion process applied to GA4 data can lead to confusing results though:            table_date      timestamp_date      pageviews                  2023-07-01      2023-07-01      43              2023-07-02      2023-07-01      1              2023-07-02      2023-07-02      24              2023-07-03      2023-07-03      186      As you can see, it appears that the event_timestamp of certain events is one day behind the event_date for July 2nd. Again, this is because the event_timestamp is expressed in UTC, while the event_date is expressed in the property’s timezone. As a result, the event_timestamp for July 2nd is actually July 1st at 22:00:00 UTC. This is because the property’s timezone is set to UTC+2 (Denmark/Copenhagen), meaning that the event_timestamp is 2 hours behind the event_date. So, our GA4 events seem to be time travelling, but in reality, it’s just a matter of timezone conversion.The GA4 Timestamp Conversion Process in BigQueryLuckily for us, the EXTRACT function allows us to adjust the event_timestamp to the property’s timezone by adding the AT TIME ZONE clause. To obtain the right property timezone for our query, we can look at the GA4 property settings in the GA4 interface:Admin &gt; Property Settings &gt; Reporting time zoneAdding the timezone to the timestamp conversion process allows us to convert the event_timestamp to any timezone (see a list of all available time zones here), which in this case is UTC+2 (Denmark/Copenhagen).Hence, after adding the AT TIME ZONE clause the resulting query looks like this:SELECT  EXTRACT(HOUR  FROM    TIMESTAMP_MICROS(event_timestamp) AT TIME ZONE \"Europe/Copenhagen\") hour_adjusted,  COUNT(*) AS pageviewsFROM  `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`WHERE  event_name = 'page_view'GROUP BY  1ORDER BY  1 ascPractical ExampleNow that we understand the conversion process, let’s apply it to a practical example to illustrate the importance of correct timestamp conversion. In this case, we’ll be looking at the event_timestamp in BigQuery and extract the DAYand the HOUR before and after adjusting it to the property’s timezone.Source: Public GA4 BQ datasetThe heatmap above shows the number of pageviews per day and hour for a GA4 property. As you can see, the heatmap is divided into two parts: the left side shows the event_timestamp before adjusting it to the property’s timezone, while the right side shows the event_timestamp after adjusting it to the property’s timezone. The difference between the two is quite significant, as the heatmap on the left shows a lot of activity in the early morning hours (8.00 - 9.00), while the heatmap on the right shows the peak of pageviews to happen later in the morning (10.00 - 11.00). Keep in mind that depending on your property’s selected time zone the effects on your decision-making process might be even more severe.Making decisions based on the heatmap on the left could lead to incorrect conclusions, and result in you missing out on valuable insights. For example, you might decide to run a campaign at 8.00 in the morning to target users who are active at that time. However, if you look at the heatmap on the right, you’ll see that the peak of pageviews happens later in the morning (10.00 - 11.00), meaning that you might be missing out on valuable traffic simply by running your campaign too early throughout the day.ConclusionAs shown in this artice, adjusting the event_timestamp for its property’s timezone is paramount when working with GA4 raw data in BigQuery. In general, I recommend using the event_timestamp for all time-related analyses and making sure to adjust it to the property’s timezone using the AT TIME ZONE clause. This will ensure that you’re working with the correct time values and avoid any unexpected discrepancies.I hope you find this article useful and that it’ll help you understand the GA4 timestamp conversion process a bit better and lead to more accurate analyses."
  },
  
  {
    "title": "Debugging Google Analytics Tracking for Mobile Apps - A Guide for Beginners",
    "url": "/gunnargriese.github.io/posts/firebase-analytics-debugging/",
    "categories": "Firebase Analytics",
    "tags": "ga4, firebase-analytics, charles-proxy, gtm, ios-debugger",
    "date": "2023-05-05 11:24:15 +0200",
    





    
    "snippet": "Are you a digital analyst looking to implement and debug Google Analytics tracking via Firebase for your mobile iOS apps and have no clue where to start? This blog post is for you! I’ll briefly wal...",
    "content": "Are you a digital analyst looking to implement and debug Google Analytics tracking via Firebase for your mobile iOS apps and have no clue where to start? This blog post is for you! I’ll briefly walk you through the process of setting up Google Analytics tracking using both the Firebase SDK and Google Tag Manager (GTM). Primarily, I’ll cover the most effective debugging tools, including Charles Proxy and David Vallejo’s iOS/Android Debugger, to help you achieve accurate and reliable data collection.Introduction to Google Analytics Tracking for AppsGoogle Analytics 4 (GA4) marks the debut of a unified platform for tracking both web and mobile app data in Google’s analytics suite. This change acknowledges the increasing trend of businesses viewing their websites and mobile apps as interconnected, rather than separate sales channels. GA4 is designed to provide a more comprehensive understanding of user interactions across various platforms.As GA4 integrates web and app analytics more closely, digital analysts must expand their knowledge of app tracking to fully leverage GA4’s potential. Traditionally, most digital analysts focused primarily on website measurement implemented via Google Tag Manager (GTM) or gtag.js.Source: https://developers.google.com/analytics/devguides/collection/protocol/ga4However, implementing and debugging GA4 for apps significantly differs from the web-based approach and is more cumbersome.There are two primary methods for enabling GA4 tracking in your mobile apps:  Using the Firebase SDK exclusively  Using the Firebase SDK in conjunction with GTM for Android/iOSFirebase SDKFirebase is a comprehensive app development platform offering various tools and services to help developers build, enhance, and grow their apps. With a direct integration into GA4 via the Analytics Software Development Kit (SDK), Firebase provides valuable insights into user behaviour and app performance.By incorporating the Firebase Analytics SDK into your app, you can harness the power of Google Analytics 4 (GA4) to track user interactions, e-commerce actions, and other crucial metrics. Developers often prefer this approach, as it offers advanced data collection and analysis capabilities, along with additional Firebase features such as Cloud Messaging and Crashlytics.Google Tag Manager for AppsGTM offers two container types for native app tagging setups: Android and iOS, designed specifically for their respective app types. Both containers feature several tag types (mainly Google-specific), as well as limited trigger and variable capabilities compared to the GTM web container.Tag options in GTM iOS containerAlthough GTM for websites is a powerful and convenient tool for enabling measurement without necessarily modifying the codebase, it is less useful for app tracking because of two main reasons:  App container capabilities are limited. For example, there is no way to inject custom code into the app via the container, and only a few ad pixels are supported (as mentioned earlier).  App containers are inconvenient from a debugging and deployment perspective. For instance, every time you make changes to your container, you need to export it, embed the exported JSON in your app, and release a new app version for your users to see the changes take effect.Consequently, fundamental changes need to be handled in the app’s source code and most specialists agree that GTM app containers should not be the “default” choice for your tracking setup — instead, there should be a valid reason to use them. However, as you might encounter these container types in real-world situations, they have been included in this overview for your reference.Set Up Google Analytics Tracking for AppsThe following section offers a high-level overview of implementing app tracking via the Firebase SDK and GTM for apps. As this article primarily focuses on the debugging process, and in practice, the SDK implementation is usually handled by app developers, we will not delve into the details.Nevertheless, it is highly recommended that you set up a test app of your own. Doing so will allow you to experience the SDK implementation process firsthand, explore Firebase’s tracking capabilities, and ultimately enhance your understanding of GA4 app tracking.Numerous tutorials are available, but this one stands out for its well-structured and easy-to-follow format:Install the Firebase SDKTo set up Google Analytics tracking using the Firebase SDK, please follow these steps (more details in the official documentation):  Create a Firebase project in the Firebase Console.  Add your iOS app to the project by providing the app’s bundle ID.  Download and integrate the Firebase SDK into your app by following the official documentation.  Enable GA4 in your Firebase project settings.  Add Consent Mode v2 to your app to have full availability of Google’s audience capabilities.  Implement event tracking in your app’s code to track specific user actions and app performance.Add Google Tag ManagerAs GTM for apps is essentially a Firebase extension it will only work if you have the Firebase SDK correctly implemented as well. Other than that, the setup flow is as follows (more details in the official documentation):  Add the Google Tag Manager package  Create an iOS or Android GTM container (if applicable)  Make modifications to your GTM workspace and publish them  Export your container version (.json file)  Add the downloaded file to your app projectEnable GA4 event trackingTo log events for GA4, use the logEvent method provided by the Firebase Analytics SDK. This method requires an event name and, optionally, a bundle of parameters to provide additional information about the event. For example, you might log a custom “purchase” event with the required parameters like transaction_id item name, price, and currency.// A pair of jeggingsvar jeggings: [String: Any] = [  AnalyticsParameterItemID: \"SKU_123\",  AnalyticsParameterItemName: \"jeggings\",  AnalyticsParameterItemCategory: \"pants\",  AnalyticsParameterItemVariant: \"black\",  AnalyticsParameterItemBrand: \"Google\",  AnalyticsParameterPrice: 9.99,  AnalyticsParameterQuantity = 2]// Prepare purchase paramsvar purchaseParams: [String: Any] = [  AnalyticsParameterTransactionID: \"T12345\",  AnalyticsParameterAffiliation: \"Google Store\",  AnalyticsParameterCurrency: \"USD\",  AnalyticsParameterValue: 14.98,  AnalyticsParameterTax: 2.58,  AnalyticsParameterShipping: 5.34,  AnalyticsParameterCoupon: \"SUMMER_FUN\"]// Add itemspurchaseParams[AnalyticsParameterItems] = [jeggings]// Log purchase eventAnalytics.logEvent(AnalyticsEventPurchase, parameters: purchaseParams)This code snippet results in a purchase event being dispatched from your app. This event will be picked up by your GTM container and you can use the “purchase” event to trigger another tag in your GTM container (s. below) and access the event parameters via built-in variables.Floodlight Tag with variables from Firebase eventFor a more detailed description of the setup process I can recommend working through Simo Ahava’s quickstart guides for Android and iOS.Debugging Google Analytics Tracking for AppsThe importance of debugging Google Analytics tracking cannot be overstated, as it ensures accurate and reliable data collection, which is crucial for building trust and making informed decisions about app development and marketing strategies. By identifying and resolving issues such as missing or inaccurate data, configuration errors, and SDK compatibility, you and your stakeholders can gain a comprehensive understanding of user behavior, app performance, and areas for improvement.In the following sections, I’ll run you through the options available. Depending on your resources and setup, I recommend choosing the most efficient one for you to debug your tagging implementation.Debugging with the DebugViewDebugView offers a valuable resource for debugging, as it allows you to view the raw event data logged by your app on development devices in near real-time. This feature is particularly useful during the instrumentation phase of development, as it helps you identify errors in your Analytics implementation and verify the correct logging of events and user properties.To activate the DebugView for your test iOS device, you need to specify the following command line argument in XCode:-FIRAnalyticsDebugEnabledTo achieve this in XCode go to Product &gt; Scheme &gt; Edit Scheme and add the argument to the Arguments passed on Launch section.Add FIRAnalyticsDebugEnabled to XCodeNow, all events dispatched from the mobile app via Firebase will be logged and can be inspected by you. You can either view the DebugView interface in the Firebase console…DebugView in Firebase Analytics…or directly in the GA4 interface.DebugView in GA4Typically, events logged by your app are batched together over a period of approximately one hour and uploaded collectively. This approach conserves battery life on users’ devices and reduces network data usage. However, for the purposes of validating your Analytics implementation and viewing your Analytics in the DebugView report, you can enable debug mode on your development device to upload events with minimal delay. This functionality is very similar to what you experience when previewing a GA4 implementation via GTM for your website where the event batching is disabled as well to allow for better debugging experience.Debugging with Network ProxiesUnlike web applications, where you can use a browser’s developer tools (e.g., console or network tab) for debugging, mobile apps do not offer the same level of accessibility to inspect and debug network requests. This limitation necessitates the use of proxy tools, to intercept and inspect network traffic, including Google Analytics requests, between the app and the server. The general functionality of such proxy tools and how they fit into the debugging process can be seen here:Debugging process with network proxy  Device: This is your mobile app or any other client-side application that sends requests and receives responses from the internet.  Proxy: This is the debugging proxy that acts as an intermediary between your mobile app and the internet. It captures, monitors, and analyzes network traffic, allowing you to inspect, modify, and record requests and responses.  Server: This represents the server-side components, such as APIs, websites, or other web resources that your mobile app communicates with.When you use such a proxy, the network traffic between your app and the internet is routed through the proxy. This allows you to intercept and display all the requests, their payloads, and responses in real-time, giving you a clear view of the data being sent and received by your app.The following tools utilize the process described above and will help you make this task much easier and ensure high data quality for your business stakeholders.Charles ProxyCharles Proxy is a web debugging proxy that allows you to monitor and analyze network traffic between your app (and any other device really) and Firebase Analytics. It’s an invaluable and free tool for identifying issues with tracking implementations, such as missing or malformed events and incorrect configuration.In the following section, I will describe how to set up the tool to use it successfully and efficiently for debugging app tracking. Please note that the examples provided are for MacOS and iOS. While the general process is similar for Windows and Android, some specifics differ.Set up Charles ProxyTo set up Charles Proxy and enable the application to intercept requests from a mobile app on your phone, please follow these steps:  Download Charles Proxy here.  Ensure that your iPhone and Macbook are on the same network  In Charles: Help &gt; SSL Proxying &gt; Install Charles Root Certificate on a Mobile Device or Remote Browser (follow the instructions on the screen)4. Configure the network connection5. Download the CA Root SSL certificate6. Install the certificate on your device7. Trust the certificate8. Decrypt requests sent over SSL: Proxy &gt; SSL Proxying Settings… &gt; *.google-analytics.com (or your sGTM endpoint); *.app-measurement.com9. Decrypt requests sent over SSL: Proxy &gt; SSL Proxying Settings… &gt; *.google-analytics.com (or your sGTM endpoint); *.app-measurement.comNow you are able to intercept and analyze all outgoing requests from your mobile app.Debug Native Mobile AppsThe Firebase SDK sends GA4 events to its own servers (app-measurement.com) in a format that is challenging to interpret.Encoded protobuf payloadInitially, the raw data appears as a series of random characters. It may seem like the content is encrypted, but Firebase actually uses Protocol buffers (“Protobuf”), a Google-developed mechanism for serializing structured data.Charles supports Protobuf out of the box, allowing you to right-click on the request and select View Request As / Protocol Buffers… to decode the data.While the data is now more readable, it is still missing substantial information. Protobuf compresses data by replacing lengthy key names with numbers, with the mapping defined in a .proto file needed for both writing and reading Protocol buffers.Unfortunately, the .proto definitions required for decoding are not made publicly available by Google. However, Lari Haataja did a great job by examining the encoded data and correlating information from other sources, and reverse engineering the names of most keys in the data structure. Check out his accompanying blog post here.Lari published the results of his research on his GitHub repository, ready for your use in Charles Proxy.To add them to your Charles Proxy configuration, open “Viewer mappings…” from the menu and add a new mapping. Add the app-measurement.desc file from Lari’s GitHub repo to the Descriptor Registry and then create a new mapping for app-measurement.com requests with the following settings:  Request type: Protocol Buffers  Message type: app_measurement.Batch  Payload encoding: Binary &amp; UnknownNow, viewing the Firebase requests displays more comprehensible values:Decoded protobuf payloadDebug WebViewsWebViews are embedded browser components within a native mobile app, allowing developers to display web content and run web-based applications directly within the app. They bridge the gap between native mobile apps and web content by providing a seamless user experience.Schematic of a WebViewUnlike native mobile apps, which are built using platform-specific programming languages like Swift for iOS or Kotlin for Android and have direct access to device features, WebViews rely on HTML, CSS, and JavaScript to create an app-like experience within the native app. While native apps can deliver better performance and tighter integration with the device’s hardware and software features, WebViews can be a cost-effective solution for developers seeking to leverage existing web-based content or simplify cross-platform development. As they are different types of app — a hybrid so to speak — the difference mainly lies within the way the Firebase Analytics SDK is implemented and not so much in the debugging process.In order to implement Firebase Analytics SDK in a WebView app, you’ll need to follow a series of steps that involve forwarding events and user properties from the WebView to the native code before sending them to Google Analytics.  Create JavaScript functions: Start by creating JavaScript functions that forward events and user properties to the native code. This should be done in a way that is compatible with both Android and Apple native code.  Implement native interface: For Apple devices, create a message handler class that conforms to the WKScriptMessageHandler protocol. Within the userContentController:didReceiveScriptMessage: callback, implement Firebase calls.  Add the message handler: Add the message handler to the WebView’s user content controller to enable communication between the JavaScript and native code.Follow the instructions provided in the Firebase Analytics WebView sample to integrate Firebase Analytics in a WebView inside an Android or iOS app.Again, using this approach you will be able to use all the debugging tools outlined in this blog post. Just be aware that there is an additional data exchange taking place between the embedded browser and the native app components.iOS/Android DebuggeriOS/Android Debugger is an application designed by David Vallejo specifically for debugging Google Analytics tracking in mobile apps. It enables you to view real-time data sent to Google Analytics, validate event tracking implementation, troubleshoot common issues, and comes with the following features:  Firebase (GA4) — The Debugger enables real-time viewing of event batches, including auto-generated events like session_start, app_background, etc., along with event parameters, user properties, audiences, and other internal payload data.  Google Tag Manager (Android only) — This work-in-progress report allows users to see which containers are loaded, which events are triggered, and the list of variables being evaluated. Note that this report is not app-based and will display all hits from the currently connected device.  Adobe Analytics (iOS only) — Users can view hits fired to any account with a clear overview of the current event name and page name.  Google Analytics (UA) — Familiar to most users, this report displays Universal Analytics (GA3) hit payloads. Note that this report is not app-based and will display all hits from the currently connected device.The setup of the iOS/Android Debugger is smooth — especially compared to Charles Proxy:  Download the application from the dedicated website  Open the application3. Select a device to debug &gt; Start Debug Session4. Configure your device and set up the proxy5. Downloading and installing the CA Root Certificate (s. Charles Proxy steps)6. Start your debugging sessionUpon completing the setup, launch the desired mobile app on your phone and navigate within the app to trigger the Firebase events you wish to debug. Wait for these events to appear on the application’s screen. The Debugger will display a sequence of events as they are registered by the proxy. By selecting a specific event, you can inspect all associated event parameters and user properties.7. View the requests sent to Google AnalyticsOther Network ProxiesApart from Charles Proxy, there are several alternative tools that offer similar capabilities for inspecting network traffic and debugging mobile apps. Fiddler is a versatile web debugging proxy that provides comprehensive network monitoring and manipulation features. Proxyman is another powerful option that simplifies the process of intercepting and editing HTTP/HTTPS requests and responses on macOS, iOS, and Android devices. Lastly, HTTP Toolkit offers an open-source solution for debugging, testing, and building with HTTP(S) on Windows, macOS, and Linux. Each of these tools operates in a similar manner, allowing you to intercept network traffic, analyze requests and responses, and gain insights into your app’s behavior, making them valuable alternatives to Charles Proxy for mobile app debugging.BigQuery Real-TimeGA4 offers real-time streaming of raw data into BigQuery, allowing you to access granular information for debugging your tracking implementation. To utilize this feature, follow these steps:  Link GA4 to BigQuery: Ensure that your GA4 property is connected to a BigQuery project. To do this, navigate to the “Admin” section in GA4, click on “BigQuery Linking” under the property column, and follow the steps to link your GA4 property to the desired BigQuery project.  Configure real-time export: Once the connection is established, enable real-time export to BigQuery by selecting the “Stream data to BigQuery in real-time” option in the linking settings.  Access real-time data in BigQuery: After enabling real-time export, navigate to your BigQuery project and look for a dataset named analytics_&lt;your_property_id&gt;. Within this dataset, you will find a table named events_intraday_&lt;datetoday&gt; that stores the raw event data streaming from GA4 in real-time.  Run queries for debugging: With the real-time data in BigQuery, you can now run SQL queries to analyze and debug your tracking implementation. For example, you can check the occurrence of specific events, validate custom parameters, or identify discrepancies in the data. This real-time access to raw data enables you to quickly identify and fix issues in your tracking setup.  Monitor and optimize: Continuously monitor the data stream and refine your queries to improve your tracking implementation. Be sure to address any discrepancies or issues you uncover during the debugging process.By utilizing GA4’s real-time data streaming into BigQuery, you can effectively debug your tracking implementation and ensure accurate data collection for your app or website.ConclusionDebugging Google Analytics tracking via Firebase for iOS apps can be challenging, particularly for beginners. This blog post aims to provide you with the necessary tools and understanding to confidently tackle mobile app tagging debugging. A thorough grasp of Firebase’s capabilities and limitations is essential for effectively navigating and leveraging the ecosystem.Debugging app tracking is notably distinct from debugging for the web. To develop proficiency in mobile app development, it’s crucial to acquaint yourself with its unique characteristics and adapt your approach accordingly.By adhering to the steps presented in this guide, you’ll be well-equipped to debug and maintain Google Analytics tracking in your mobile app, yielding valuable insights into user behavior and app performance. Happy debugging!"
  },
  
  {
    "title": "Unleash the Potential of PostHog Analytics Platform with Google Tag Manager",
    "url": "/gunnargriese.github.io/posts/posthog-gtm-template/",
    "categories": "GTM",
    "tags": "gtm, custom-templates, posthog",
    "date": "2023-04-10 08:24:15 +0200",
    





    
    "snippet": "In today’s data-driven world, understanding user behavior is crucial for any business. Product analytics platforms like PostHog have emerged as essential tools for gaining insights into how users i...",
    "content": "In today’s data-driven world, understanding user behavior is crucial for any business. Product analytics platforms like PostHog have emerged as essential tools for gaining insights into how users interact with your website or app. In this blog post, we will explore the PostHog analytics platform, its SDKs, the JavaScript SDK in particular, Google Tag Manager tag templates, and how you can integrate PostHog with Google Tag Manager using a custom tag template.Introduction to PostHog Analytics PlatformPostHog is an open-source product analytics platform that helps you analyze user behavior on your website or app. It provides event-based tracking, funnel analysis, user segmentation, and other essential features to help understand how users engage with the product you are responsible for. It sits in the same space as tools like Mixpanel, Heap, and Amplitude which are all specializing in providing product managers and analysts with the required insights to optimize their platform towards their users’ needs.If you are looking for a deep-dive on how Product Analytics differs from Marketing Analytics, I recommend checking out Adam Greco’s blog series on that topic.PostHog SDKs for Data CollectionPostHog itself offers a variety of SDKs (Software Development Kits) to make it easy for you to collect data from different platforms:  JavaScript: For tracking user interactions on websites.  Python: For backend applications or data pipelines.  Ruby: For Ruby on Rails applications.  PHP: For PHP-based applications.  Go: For applications written in Go.  Node.js: For server-side JavaScript applications.  Android: For Android apps.  iOS: For iOS apps.These SDKs enable you to implement PostHog tracking in your application or website, regardless of the technology stack you’re using.Details on the JavaScript SDKThe PostHog JavaScript SDK is the most widely used SDK for tracking user interactions on websites. It provides a simple API to send events, identify users, and manage user properties. You can track custom events, page views, clicks, form submissions, and more with minimal code.The SDK also supports advanced features like autocapture, which automatically tracks user interactions with your website without requiring additional code. You can configure the SDK to suit your needs, such as disabling certain features or customizing how data is captured.Google Tag Manager Custom TemplatesAs I personally spend most of my time working with Google Tag Manager (GTM) and Google Analytics (GA), GTM is actually my preferred tool when it comes to implementing JS tracking code on websites or web apps and is widely used. If your organization is not only engaging in product analytics, but also actively running digital marketing campaigns, there is a high chance that your product has GTM embedded already.Google Tag Manager (GTM) is a powerful tool that allows you to manage and deploy marketing tags, analytics tracking, and other code snippets on your website without the need for manual code updates. GTM uses tag templates, which are pre-built code snippets that can be customized to suit your specific tracking requirements.Tag templates help streamline the process of implementing third-party tracking scripts, like PostHog, on your website. They allow you to configure tracking settings without editing code directly, making it easier to manage and maintain your tracking implementations. These templates can be submitted to the GTM Template Gallery and are then available to everyone who would like to use them.The catch is that PostHog has not yet published an official template to run their JS SDK through GTM. That is why I set out to bring (most of) the functionalities of PostHog’s JS SDK to GTM without the need to deploy any (potentially harmful) custom HTML tags.Integrating PostHog with Google Tag Manager Using the Provided TemplateThe provided PostHog GTM tag template makes it simple to integrate PostHog tracking into your GTM container.Custom Event with event parameters implementation using the GTM templateWith this template, you can manage events, user properties, and configuration settings directly from your GTM workspace. Here’s what you can do with the template:  Initialize PostHog with various configuration options.  Capture events and virtual pageviews with custom properties.  Identify users with user properties.See below for some examples of the GTM template functionalities.Initialization — Basic ConfigurationExemplary configuration of the Initialization tagSee detailed documentation for each of the configuration options here. The configuration options represented in the tag template are:      apiHost: The domain of your PostHog instance (e.g., https://app.posthog.com). The tag will append /static/array.js to this value to build the actual request URL.    apiKey: Your PostHog project API key  autocapture_tuning: Custom allowlist settings for autocapture.  autocapture_off: Disable autocapture completely .  capture_pageview: Enable/disable capturing pageviews.  capture_pageleave: Enable/disable capturing page leave events.  cross_subdomain_cookie: Enable/disable cross-subdomain cookie tracking.  disable_persistence: Enable/disable cookie/localStorage persistence.  disable_session_recording: Enable/disable session recording.  enable_recording_console_log: Enable/disable console log recording.  mask_all_text: Enable/disable masking all text.Initialization — Advanced Tracker ConfigurationExemplary callback functionThe PostHog library can be initiated with a callback (e.g., call identify) that is executed once the library has loaded.For the callback function to work in the GTM template version, you need to reference a custom JS variable in the field loaded (s. screenshot in section Basic Configuration) that returns a function.As soon as the PostHog scripts are handled and the tracker is initiated, this function will be executed.Custom Events, User Properties, and Event ParametersExemplary custom event tag implementationUsing the GTM template, you can effortlessly track custom events (as well as well virtual pageviews) and augment them with fine-grained event parameters, providing a comprehensive understanding of user engagement patterns. Furthermore, the template adopted the JavaScript SDK functionality which allow you to set user properties using both $set and $set_once methods, giving you the flexibility to choose between updating values or preserving the initial user attributes.How to get started?Please feel free to download the template from my GitHub and give it a spin. I am happy for any feedback to improve the template, include more functionalities from the original SDK, and fix potential bugs as they arise.To use the template, simply import it into your GTM workspace, create a new tag using the template, and configure the settings based on your needs. This integration ensures that you can take full advantage of both PostHog and Google Tag Manager to gain insights into user behavior on your website or app.While this article outlines an approach on how to implement PostHog through GTM, you might want to refer to PostHog’s great and easy-to-follow documentation on how to get started with their platform.SummaryIn conclusion, PostHog is a powerful analytics platform that can help you understand your users better. By integrating it with Google Tag Manager using the provided tag template, you can streamline the implementation process and efficiently manage your tracking setup. This combination of PostHog and Google Tag Manager will hopefully help speeding up implementation projects and empower you to make data-driven decisions even faster. So go ahead, explore the potential of PostHog analytics platform and Google Tag Manager integration, and unlock new insights into your users’ behavior."
  },
  
  {
    "title": "How to replicate the GA4 Path Exploration report with BigQuery SQL?",
    "url": "/gunnargriese.github.io/posts/bigquery-path-exploration/",
    "categories": "BigQuery",
    "tags": "ga4, bigquery, sql",
    "date": "2022-10-23 11:24:15 +0200",
    





    
    "snippet": "The New Google Analytics (GA4) comes with a new Exploration feature, which allows GA users to deep-dive into their data — beyond the capabilities of the built-in standard reports. While the standar...",
    "content": "The New Google Analytics (GA4) comes with a new Exploration feature, which allows GA users to deep-dive into their data — beyond the capabilities of the built-in standard reports. While the standard reports allow for monitoring key business metrics, the Exploration section makes advanced analytical techniques accessible to generate ad hoc insights quickly.The analytical techniques available are the following:  Free-form exploration  Cohort exploration  Funnel exploration  Segment overlap  User exploration  User lifetime  Path explorationCheck out the official Google documentation for a more detailed introduction to GA4’s powerful Explorations feature.Path Exploration in GA4The Path exploration report lets you visualize user interactions from a specific event/page forward (starting point) or backward (ending point) in an aggregated manner. Analysts using this technique can answer questions like:  What top pages do new users open after visiting the home page?  What is the effect of an event on subsequent user actions?  Which pages have broken links to 404 pages?Elements of Path exploration (https://support.google.com/analytics/answer/9317498?hl=en&amp;ref_topic=9266525)The visualization is a Sankey diagram, where each node represents an event or page. Analysts can arbitrarily add nodes to reveal more sequence elements and apply segments, filters, and breakdown dimensions to tailor the report to their needs.Turning to BigQuery to extract the dataWhile this analysis technique is powerful, it is not always the ideal tool to share these insights with stakeholders — especially to those unfamiliar with GA. Luckily, with GA4, all users have the opportunity to export their GA4 data into BigQuery (Google’s cloud-based, fully-managed database designed for analytical workloads).To constantly monitor critical user flows and embed them into your standard reporting tools (e.g., Looker Studio, Power BI, or Tableau), turning to the GA4 raw data in BigQuery and extracting the data is the way to go. But…How to replicate a basic GA4 Path Exploration report in BigQuery?We can utilize window functions in BigQuery to aggregate the page_location sequences across all sessions measured with GA4 with a query like this:Query example for path exploration with a starting pointThe query above will return one row for each path and the number of occurrences within the queried table:Query result for path exploration with starting pointThe query’s core components are the following:  ga4EventParams function  LEAD() function over a session WINDOW  Temporary table specified in a WITH clauseUser-defined functions to increase readabilitySince we are interested in obtaining the page_location for each associated page_view event, page_location is the key we provide as input to the function. The ga4EventParams function returns the respective value for the input key from the event_params RECORD. For more details, check out Alejandro’s posts on how to use a BigQuery function.Window functions to query sequencesThe LEAD() navigation function allows us to query subsequent rows from a specified window. Changing the offset value alters which subsequent row is returned; the default value is 1, indicating the next row in the window frame. You can use the offset value to add more nodes to your query, depending on your analysis. In the example above, 3 additional nodes have been included (besides the starting point).The WINDOW clause above will result in the window spanning over a user’s session. To make the window user-scoped, remove the ga_session_id reference.Temporary tables to store intermediate resultsThe temporary table allows us to store the resulting table — 1 row per page_view event with the subsequent page_locations as columns — as an intermediate query result in memory (data) and make it available to a later part of the query. The data table is then used in the final FROM clause to count the occurrences of each sequence across all sessions.How to look back in time?  All nice and dandy, but how to start the sequence with its ending and work our way back from there?Fair question, especially since specifying the ending point and analyzing how users ended up there is a crucial function of GA4’s Path exploration technique.Luckily, this would require just a few minor tweaks within our temporary table, like so:Query example for path exploration with an ending pointBy simply replacing LEAD() with LAG(), we return the value of the page_location on a preceding row. So, the previous page_view event of a session.How to start the sequence with the actual landing page?To achieve this, we need to, once again, turn to our familiar LEAD() function and add a column to our temporary table — the entrances event parameter. It will indicate whether or not a certain page_view event was the first within a session and, therefore, is the landing page.Filtering for sequences that start on the landing page will return the desired result:Query example for path exploration with the landing page as a starting pointFinal remarksI hope this piece of context was helpful for you and encourages you to dive deeper into GA4’s BigQuery raw data. From my perspective, having access to the raw data is one of the significant gains you get from migrating from Universal Analytics to GA4.I intend to identify other use cases to explore the opportunities and limitations of GA4 and BigQuery.So, if you discover any flaws in the queries, have remarks or find anything unclear, please contact me. I am always happy to talk analytics!"
  },
  
  {
    "title": "GTM Server-Side Tagging – Better Data & More Control",
    "url": "/gunnargriese.github.io/posts/gtm-server-side/",
    "categories": "GTM",
    "tags": "ga4, gtm-server-side",
    "date": "2020-08-21 15:24:15 +0200",
    





    
    "snippet": "Over the last many months we have been running a series of beta tests and projects for a group of selected clients to get both practical experience and to help optimize the recently released Google...",
    "content": "Over the last many months we have been running a series of beta tests and projects for a group of selected clients to get both practical experience and to help optimize the recently released Google Tag Manager (GTM) server-side tagging setup.Our ambition is to provide both background information and context to why we think most businesses should seriously consider adopting it to better support the business. Moreover, you will get an overview of the feature’s functionality and possible use cases to move towards a future-proven, first-party tracking setup, improve overall customer experience, and gain full control over your data.The main takeaway is that GTM server-side will increase flexibility and control over your data and enhance the durability and the direct business value of the solution far beyond what is possible with a traditional Google Tag Manager setup.Privacy First and 1st Party AnalyticsNowadays businesses face many challenges that require them to rethink the way they do website and app tracking. Luckily, these challenges with GTM server-side will also open up many agile opportunities for companies willing to adapt and change. However, traditional tracking is being confronted by a lot of challenges that will make “tracking as usual” technically impossible.One of these challenges is the new “Privacy first”-paradigm imposed upon us by regulations like GDPR (EU) and CCPA (California). Browser providers have also become much more privacy-aware within the last couple of years, seeking to better protect their users’ data from all too data greedy adtech tools.Similarly, initiatives like Safari’s “Intelligent Tracking Prevention” (ITP) and Firefox’s “Enhanced Tracking Prevention” (ETP) heavily restrict access to frequently used browser storage options (esp. cookies and local storage). Google also just recently announced that they will phase out third-party cookies altogether within the next two years for their Chrome browser (a so-called “privacy sandbox” will replace third-party cookies).Too often advertisers have also implemented Javascript snippets on their websites without being aware of its full functionality, resulting in handing over control over their users’ data to unknown third-parties and making massive data breaches possible in the first place.Therefore, we should welcome the general direction of these actions taken since it helps to secure our personal data, to mature the industry, and to regain long-lost credibility – aiming to end questionable “wild wild west”-tracking practices.The main downside to this movement is that it also affects and creates biases in business-critical data collection based on user consent: The technical and regulatory environment that traditional website and app tracking are embedded in makes it difficult for us to keep track of specific user behavior – increasing our blindspot in understanding a user’s needs.GTM Server-Side Tagging to the RescueGoogle Tag Manager (GTM) now comes with a new feature (in beta status for the moment) that will enable businesses to create a 1st-party analytics setup. GTM server-side tagging has arrived to change how website and app tracking are implemented, and how data is shared with third-parties. Follow along to understand how this feature will help you improve website user experience, increase security of your users’ data, and increase control over your data collection.How does it work?While the GTM server-side container provides new tools and features to measure user activity, the general model of tags, triggers, and variables remains untouched.Source: https://developers.google.com/tag-platform/tag-manager/server-side/introGTM Server-side tagging translates to a new “Server” container running on an App Engine instance – (ideally) mapped to your domain namespace – in your own Google Cloud Platform (GCP) project. It is then possible to send HTTP requests to the server from the user’s device or any other device with an internet connection. The GTM server turns those requests into events that are processed by the container’s tags, triggers, and variables.While the well-known concept of tags, triggers, and variables work as they used to on client-side GTM containers, the newly introduced “clients” is the glue between the devices sending requests and the container logic. The client can receive and claim requests (and data sent with it), parses them into one or more events, routes data to be processed in the container (tags, triggers, and variables), and returns an HTTP response to the requesting device.Gain Data Control and FlexibilityWhat is different about this approach than tracking based on client-side Javascript and pixels, is that you have full control and gain flexibility over how the data is processed until you send it to third-party tools (enabling hit validation, PII controls, etc.). In fact, since the GTM server (https://gtm.example.com) is associated with the same domain as your website (https://example.com), every interaction between them is considered to be a first-party data exchange by the user’s browser.Mitigate the Impact of Browser Tracking Prevention and Ad BlockersBy this change of context, significant restrictions forced upon us by browser tracking prevention will be circumvented, because you gain the ability to set first-party HTTP cookies (e.g., GTM server-side introduces a new FPID cookie). For example, now Safari’s ITP will no longer restrict the lifetime of cookies associated with Google Analytics when placed from the server-side resulting in better data quality for reporting and analysis (especially relevant for businesses with a young and tech-savvy user base). Keep in mind that this power comes with great responsibility, and you should always be aware of the legal and ethical implications of setting cookies and the data you collect.What Use Cases Can Be Revealed?GTM Server-side tracking restores transparency in third-party tool and website usage, but beyond that, there are a few other advantages when you track events on the server-side that can lead to strong use cases for the business.Improved Data Accuracy and ControlData accuracy can be improved even more when businesses decide to leverage server-to-server communication for business-critical events like transactions. We have all experienced discrepancies between the number of transactions in Google Analytics (GA) and CRM systems due to ad blockers and other side-effects when relying on client-side tracking (page reloads, etc.). This can be mitigated by removing client-side tracking of transactions and sending the required data points directly from your CRM systems to the GTM server, passing it on to GA for analysis.Improved Page Load Speed and Data SecurityIn general, server-side tracking can remove the processing burden from a given browser and move it to the cloud. Since one HTTP request is enough to trigger an event in the server-side container, it can trigger multiple tags in the server-side environment. For example, the client could send one HTTP request on every page load, triggering a pageview hit for GA and Facebook at the same time. Given that no Javascript written by Facebook would need to be executed in the user’s browser, with server-side tracking it may be possible to increase page load speed. Since this will positively affect overall user experience, this has the potential to increase your conversion rates.Smooth Integration to the Google Cloud PlatformFurthermore, because the GTM container is executed in Google’s cloud integrations, other GCP resources like BigQuery, ML Engine, and Cloud Functions will most likely be integrated soon. This will open up a lot of possibilities for advanced use cases involving machine learning and event-based analytics.Once this new GTM feature moves out of the beta phase after being improved based on feature requests and active community members’ contributions, even more possibilities will be available, significantly changing tracking implementations based on GTM.  Note: You can read more about the possibilities of integrating GCP services into GTM Server-Side to unlock powerful use cases in the future in my latest blog post series.Closing ThoughtsThe GTM server-side container’s release continues a general trend that can be observed for many Google Marketing Platform tools: Strengthen tool integration and push towards GCP. By pursuing this strategy, Google ensures a smooth data flow between systems and, at the same time, increases each tool’s functionality to cope with the ever-changing adtech environment."
  }
  
]

